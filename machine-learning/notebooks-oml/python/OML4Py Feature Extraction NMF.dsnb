[{"layout":null,"template":null,"templateConfig":null,"name":"OML4Py Feature Extraction NMF","description":null,"readOnly":false,"type":"low","paragraphs":[{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":null,"title":null,"message":[],"enabled":true,"result":{"startTime":1737152689953,"interpreter":"md.low","endTime":1737152690426,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":true,"width":0,"hideResult":true,"dynamicFormParams":"{}","row":0,"hasTitle":false,"hideVizConfig":true,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":null,"message":["%md","# OML4Py Feature Extraction: Non-Negative Matrix Factorization (NMF)","","In this notebook, we demonstrate how to perform dimensionality reduction using the in-database non-negative matrix factorization algorithm. In this example, we illustrate the use of NMF to extract features from the Iris dataset.","","Non-Negative Matrix Factorization is a state-of-the-art feature extraction algorithm. NMF is useful when there are many attributes and the individual attributes have weak predictability. NMF derives new features as linear combinations of the original attributes, which can be used as input to other algorithms or to produce meaningful patterns, topics, or themes. ","","Each feature has a set of coefficients that are a measure of the weight each attribute conributes to the feature. There is a separate coefficient for each numerical attribute and for each distinct value of each categorical attribute. The coefficients are all non-negative, which makes it easier to interpret.","","### Matrix Factorization","","NMF uses techniques from multivariate analysis and linear algebra. It decomposes the data as a matrix *M* into the product of two lower ranking matrices *W* and *H*. The sub-matrix *W* contains the NMF basis; the sub-matrix *H* contains the associated coefficients (weights).","","The algorithm iteratively modifies of the values of *W* and *H* so that their product approaches *M*. The technique preserves much of the structure of the original data and guarantees that both basis and weights are non-negative. The algorithm terminates when the approximation error converges, or a specified number of iterations is reached.","","The NMF algorithm must be initialized with a seed to indicate the starting point for the iterations. Because of the high dimensionality of the processing space and the fact that there is no global minimization algorithm, the appropriate initialization can be critical in obtaining meaningful results. OML4Py uses a random seed that initializes the values of *W* and *H* based on a uniform distribution. This approach works well in most cases.","","#### Scoring with NMF","NMF can be used as a pre-processing step for dimensionality reduction in classification, regression, clustering, and other machine learning tasks. Scoring using an NMF model produces data projections in the new feature space. The magnitude of a projection indicates how strongly a record maps to a feature.","","#### Text Analysis with NMF","NMF is well-suited for analyzing text. In a text document, the same word can occur in different places with different meanings. For example, \"hike\" can be applied to the outdoors or to interest rates. By combining attributes, NMF introduces context, which is essential for explanatory power:","* \"hike\" + \"mountain\" -> \"outdoor sports\"","* \"hike\" + \"interest\" -> \"interest rates\"","","","Copyright (c) 2025 Oracle Corporation ","###### <a href=\"https://oss.oracle.com/licenses/upl/\" target=\"_blank\">The Universal Permissive License (UPL), Version 1.0<\/a>","---"],"enabled":true,"result":{"startTime":1737152690914,"interpreter":"md.low","endTime":1737152691375,"results":[{"message":"<h1 id=\"oml4py-feature-extraction-non-negative-matrix-factorization-nmf\">OML4Py Feature Extraction: Non-Negative Matrix Factorization (NMF)<\/h1>\n<p>In this notebook, we demonstrate how to perform dimensionality reduction using the in-database non-negative matrix factorization algorithm. In this example, we illustrate the use of NMF to extract features from the Iris dataset.<\/p>\n<p>Non-Negative Matrix Factorization is a state-of-the-art feature extraction algorithm. NMF is useful when there are many attributes and the individual attributes have weak predictability. NMF derives new features as linear combinations of the original attributes, which can be used as input to other algorithms or to produce meaningful patterns, topics, or themes.<\/p>\n<p>Each feature has a set of coefficients that are a measure of the weight each attribute conributes to the feature. There is a separate coefficient for each numerical attribute and for each distinct value of each categorical attribute. The coefficients are all non-negative, which makes it easier to interpret.<\/p>\n<h3 id=\"matrix-factorization\">Matrix Factorization<\/h3>\n<p>NMF uses techniques from multivariate analysis and linear algebra. It decomposes the data as a matrix <em>M<\/em> into the product of two lower ranking matrices <em>W<\/em> and <em>H<\/em>. The sub-matrix <em>W<\/em> contains the NMF basis; the sub-matrix <em>H<\/em> contains the associated coefficients (weights).<\/p>\n<p>The algorithm iteratively modifies of the values of <em>W<\/em> and <em>H<\/em> so that their product approaches <em>M<\/em>. The technique preserves much of the structure of the original data and guarantees that both basis and weights are non-negative. The algorithm terminates when the approximation error converges, or a specified number of iterations is reached.<\/p>\n<p>The NMF algorithm must be initialized with a seed to indicate the starting point for the iterations. Because of the high dimensionality of the processing space and the fact that there is no global minimization algorithm, the appropriate initialization can be critical in obtaining meaningful results. OML4Py uses a random seed that initializes the values of <em>W<\/em> and <em>H<\/em> based on a uniform distribution. This approach works well in most cases.<\/p>\n<h4 id=\"scoring-with-nmf\">Scoring with NMF<\/h4>\n<p>NMF can be used as a pre-processing step for dimensionality reduction in classification, regression, clustering, and other machine learning tasks. Scoring using an NMF model produces data projections in the new feature space. The magnitude of a projection indicates how strongly a record maps to a feature.<\/p>\n<h4 id=\"text-analysis-with-nmf\">Text Analysis with NMF<\/h4>\n<p>NMF is well-suited for analyzing text. In a text document, the same word can occur in different places with different meanings. For example, &quot;hike&quot; can be applied to the outdoors or to interest rates. By combining attributes, NMF introduces context, which is essential for explanatory power:<\/p>\n<ul>\n<li>&quot;hike&quot; + &quot;mountain&quot; -&gt; &quot;outdoor sports&quot;<\/li>\n<li>&quot;hike&quot; + &quot;interest&quot; -&gt; &quot;interest rates&quot;<\/li>\n<\/ul>\n<p>Copyright (c) 2025 Oracle Corporation<\/p>\n<h6 id=\"the-universal-permissive-license-upl-version-10\"><a href=\"https://oss.oracle.com/licenses/upl/\" target=\"_blank\">The Universal Permissive License (UPL), Version 1.0<\/a><\/h6>\n<hr />\n","type":"HTML"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":true,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":false,"hideVizConfig":true,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"For more information...","message":["%md","","* <a href=\"https://docs.oracle.com/en/cloud/paas/autonomous-data-warehouse-cloud/index.html\" target=\"_blank\">Oracle ADB Documentation<\/a>","* <a href=\"https://github.com/oracle-samples/oracle-db-examples/tree/main/machine-learning\" target=\"_blank\">OML folder on Oracle GitHub<\/a>","* <a href=\"https://www.oracle.com/machine-learning\" target=\"_blank\">OML Web Page<\/a>","* <a href=\"https://docs.oracle.com/en/database/oracle/machine-learning/oml4sql/23/dmcon/non-negative-matrix-factorization.html\" target=\"_blank\">OML NMF Documentation<\/a>","* <a href=\"https://docs.oracle.com/en/database/oracle/machine-learning/oml4py/2/mlugp/non-negative-matrix-factorization-model.html\" target=\"_blank\">OML4Py NMF Documentation<\/a>"],"enabled":true,"result":{"startTime":1737152691859,"interpreter":"md.low","endTime":1737152692317,"results":[{"message":"<ul>\n<li><a href=\"https://docs.oracle.com/en/cloud/paas/autonomous-data-warehouse-cloud/index.html\" target=\"_blank\">Oracle ADB Documentation<\/a><\/li>\n<li><a href=\"https://github.com/oracle-samples/oracle-db-examples/tree/main/machine-learning\" target=\"_blank\">OML folder on Oracle GitHub<\/a><\/li>\n<li><a href=\"https://www.oracle.com/machine-learning\" target=\"_blank\">OML Web Page<\/a><\/li>\n<li><a href=\"https://docs.oracle.com/en/database/oracle/machine-learning/oml4sql/23/dmcon/non-negative-matrix-factorization.html\" target=\"_blank\">OML NMF Documentation<\/a><\/li>\n<li><a href=\"https://docs.oracle.com/en/database/oracle/machine-learning/oml4py/2/mlugp/non-negative-matrix-factorization-model.html\" target=\"_blank\">OML4Py NMF Documentation<\/a><\/li>\n<\/ul>\n","type":"HTML"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":true,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":true,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"Import libraries","message":["%python","","import oml","import pandas as pd","import numpy as np","from sklearn import datasets ","import warnings","","warnings.simplefilter(action='ignore', category=FutureWarning)"],"enabled":true,"result":{"startTime":1737152692800,"interpreter":"python.low","endTime":1737152693271,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"Create table containing dataset iris from sklearn and generate a unique identifier column","message":["%python","","iris = datasets.load_iris()","x = pd.DataFrame(iris.data, columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width'])","x.insert(0, \"ID\", range(1, len(x) + 1))","y = pd.DataFrame(list(map(lambda x: {0: 'setosa', 1: 'versicolor', 2:'virginica'}[x], iris.target)), columns = ['Species'])","","try: ","  oml.drop(table='IRIS_NMF')","except: ","  print(\"Table IRIS_NMF not found\")","","IRIS = oml.create(pd.concat([x, y], axis=1), table = 'IRIS_NMF')"],"enabled":true,"result":{"startTime":1737152693737,"interpreter":"python.low","endTime":1737152694274,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"Create training data and test data","message":["%python","","IRIS_SPLIT = IRIS.split()","IRIS_TRAIN = IRIS_SPLIT[0]","IRIS_TEST  = IRIS_SPLIT[1]"],"enabled":true,"result":{"startTime":1737152694736,"interpreter":"python.low","endTime":1737152695636,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"ODMS_DETAILS","message":["%md","","### Reducing the size of the resulting model with `ODMS_DETAILS`","","By disabling the `ODMS_DETAILS` setting, you can reduce the space consumed by the model, especially a partitioned model. The default value is ODMS_ENABLE.","","When the setting is ODMS_ENABLE, it creates model tables and views associated with the model. You can query the tables with SQL.","","Compare the content of the model objects with and without model details below. "],"enabled":true,"result":{"startTime":1737152696093,"interpreter":"md.low","endTime":1737152696535,"results":[{"message":"<h3 id=\"reducing-the-size-of-the-resulting-model-with-odms_details\">Reducing the size of the resulting model with <code>ODMS_DETAILS<\/code><\/h3>\n<p>By disabling the <code>ODMS_DETAILS<\/code> setting, you can reduce the space consumed by the model, especially a partitioned model. The default value is ODMS_ENABLE.<\/p>\n<p>When the setting is ODMS_ENABLE, it creates model tables and views associated with the model. You can query the tables with SQL.<\/p>\n<p>Compare the content of the model objects with and without model details below.<\/p>\n","type":"HTML"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":true,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":false,"hideVizConfig":true,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"Create NMF model with model details on the training data","message":["%python","","nmf_mod_details = oml.nmf(ODMS_DETAILS = 'ODMS_ENABLE')","nmf_mod_details = nmf_mod_details.fit(IRIS_TRAIN)"],"enabled":true,"result":{"startTime":1737152697020,"interpreter":"python.low","endTime":1737152698915,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":6,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"Create NMF model without model details on the training data","message":["%python","","nmf_mod_no_details = oml.nmf(ODMS_DETAILS = 'ODMS_DISABLE')","nmf_mod_no_details = nmf_mod_no_details.fit(IRIS_TRAIN)"],"enabled":true,"result":{"startTime":1737152699376,"interpreter":"python.low","endTime":1737152700347,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":6,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":"[{\"raw\":{\"height\":300,\"lastColumns\":[],\"version\":1}}]","hideInIFrame":false,"selectedVisualization":"raw","title":"Show model with model details enabled","message":["%python","","nmf_mod_details"],"enabled":true,"result":{"startTime":1737152700844,"interpreter":"python.low","endTime":1737152701429,"results":[{"message":"\nAlgorithm Name: Non-Negative Matrix Factorizationx\n\nMining Function: FEATURE_EXTRACTION\n\nSettings: \n                   setting name                   setting value\n0                     ALGO_NAME  ALGO_NONNEGATIVE_MATRIX_FACTOR\n1           NMFS_CONV_TOLERANCE                             .05\n2      NMFS_NONNEGATIVE_SCORING      NMFS_NONNEG_SCORING_ENABLE\n3           NMFS_NUM_ITERATIONS                              50\n4              NMFS_RANDOM_SEED                              -1\n5                  ODMS_DETAILS                     ODMS_ENABLE\n6  ODMS_MISSING_VALUE_TREATMENT         ODMS_MISSING_VALUE_AUTO\n7                 ODMS_SAMPLING           ODMS_SAMPLING_DISABLE\n8                     PREP_AUTO                              ON\n\nComputed Settings: \n          setting name setting value\n0    FEAT_NUM_FEATURES             2\n1  NMFS_NUM_ITERATIONS             2\n\nGlobal Statistics: \n  attribute name attribute value\n0      CONVERGED             YES\n1     CONV_ERROR       0.0444448\n2     ITERATIONS               2\n3       NUM_ROWS             111\n4    SAMPLE_SIZE             111\n\nAttributes: \nID\nPetal_Length\nPetal_Width\nSepal_Length\nSepal_Width\nSpecies\n\nPartition: NO\n\nH: \n\n    FEATURE_ID  FEATURE_NAME ATTRIBUTE_NAME ATTRIBUTE_VALUE  COEFFICIENT\n0            1             1             ID            None     0.581551\n1            1             1   Petal_Length            None     0.355323\n2            1             1    Petal_Width            None     0.158492\n3            1             1   Sepal_Length            None     0.656558\n4            1             1    Sepal_Width            None     0.424101\n5            1             1        Species          setosa     0.089560\n6            1             1        Species      versicolor     0.534806\n7            1             1        Species       virginica     0.539590\n8            2             2             ID            None     0.344647\n9            2             2   Petal_Length            None     0.506623\n10           2             2    Petal_Width            None     0.650077\n11           2             2   Sepal_Length            None     0.170237\n12           2             2    Sepal_Width            None     0.248640\n13           2             2        Species          setosa     0.249221\n14           2             2        Species      versicolor     0.042316\n15           2             2        Species       virginica     0.093861\n\nW: \n\n    FEATURE_ID  FEATURE_NAME ATTRIBUTE_NAME ATTRIBUTE_VALUE  COEFFICIENT\n0            1             1             ID            None     0.288559\n1            1             1   Petal_Length            None    -0.062579\n2            1             1    Petal_Width            None    -0.370128\n3            1             1   Sepal_Length            None     0.502382\n4            1             1    Sepal_Width            None     0.212611\n5            1             1        Species      versicolor     0.486970\n6            1             1        Species          setosa    -0.113835\n7            1             1        Species       virginica     0.450038\n8            2             2             ID            None     0.119462\n9            2             2   Petal_Length            None     0.578697\n10           2             2    Petal_Width            None     0.982575\n11           2             2   Sepal_Length            None    -0.238993\n12           2             2    Sepal_Width            None     0.082511\n13           2             2        Species          setosa     0.353453\n14           2             2        Species      versicolor    -0.359264\n15           2             2        Species       virginica    -0.275074\n\n\n","type":"TEXT"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":6,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":"[{\"raw\":{\"height\":300,\"lastColumns\":[],\"version\":1}}]","hideInIFrame":false,"selectedVisualization":"raw","title":"Show model with model details disabled","message":["%python","","nmf_mod_no_details"],"enabled":true,"result":{"startTime":1737152701900,"interpreter":"python.low","endTime":1737152702398,"results":[{"message":"\nAlgorithm Name: Non-Negative Matrix Factorizationx\n\nMining Function: FEATURE_EXTRACTION\n\nSettings: \n                   setting name                   setting value\n0                     ALGO_NAME  ALGO_NONNEGATIVE_MATRIX_FACTOR\n1           NMFS_CONV_TOLERANCE                             .05\n2      NMFS_NONNEGATIVE_SCORING      NMFS_NONNEG_SCORING_ENABLE\n3           NMFS_NUM_ITERATIONS                              50\n4              NMFS_RANDOM_SEED                              -1\n5                  ODMS_DETAILS                    ODMS_DISABLE\n6  ODMS_MISSING_VALUE_TREATMENT         ODMS_MISSING_VALUE_AUTO\n7                 ODMS_SAMPLING           ODMS_SAMPLING_DISABLE\n8                     PREP_AUTO                              ON\n\nAttributes: \nID\nPetal_Length\nPetal_Width\nSepal_Length\nSepal_Width\nSpecies\n\nPartition: NO\n\n\n","type":"TEXT"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":6,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":null,"message":["%md\r","\r","#### Making Predictions\r","\r","Use `predict(x, supplemental_cols=None)` to make predictions on new data.\r","\r","**Parameters**\r","* `x` [an OML object] Attribute values used by the models to generate scores.\r","* `supplemental_cols` [oml.DataFrame, oml.Float, oml.String, or None (default)] Data set presented with the prediction result. It must be concatenatable with `x`.\r","\r","**Returns**\r","* `pred` [oml.DataFrame] Contains the predicted feature index on the new data and the specified `supplemental_cols`.\r"],"enabled":true,"result":{"startTime":1737152702868,"interpreter":"md.low","endTime":1737152703326,"results":[{"message":"<h4 id=\"making-predictions\">Making Predictions<\/h4>\n<p>Use <code>predict(x, supplemental_cols=None)<\/code> to make predictions on new data.<\/p>\n<p><strong>Parameters<\/strong><\/p>\n<ul>\n<li><code>x<\/code> [an OML object] Attribute values used by the models to generate scores.<\/li>\n<li><code>supplemental_cols<\/code> [oml.DataFrame, oml.Float, oml.String, or None (default)] Data set presented with the prediction result. It must be concatenatable with <code>x<\/code>.<\/li>\n<\/ul>\n<p><strong>Returns<\/strong><\/p>\n<ul>\n<li><code>pred<\/code> [oml.DataFrame] Contains the predicted feature index on the new data and the specified <code>supplemental_cols<\/code>.<\/li>\n<\/ul>\n","type":"HTML"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":true,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":false,"hideVizConfig":true,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":"[{\"raw\":{\"height\":300,\"lastColumns\":[],\"version\":1}}]","hideInIFrame":false,"selectedVisualization":"table","title":"Use the model to make predictions on test data","message":["%python","","z.show(nmf_mod_details.predict(IRIS_TEST, ","                               supplemental_cols = IRIS_TEST[:, ","                                                             ['Sepal_Length', ","                                                             'Sepal_Width', ","                                                             'Species']]))"],"enabled":true,"result":{"startTime":1737152703809,"interpreter":"python.low","endTime":1737152704368,"results":[{"message":"Sepal_Length\tSepal_Width\tSpecies\tFEATURE_ID\n5.0\t3.6\tsetosa\t2\n5.0\t3.4\tsetosa\t2\n4.4\t2.9\tsetosa\t2\n4.9\t3.1\tsetosa\t2\n4.8\t3.4\tsetosa\t2\n5.4\t3.9\tsetosa\t2\n5.2\t3.5\tsetosa\t2\n5.2\t3.4\tsetosa\t2\n4.4\t3.0\tsetosa\t2\n5.1\t3.4\tsetosa\t2\n4.8\t3.0\tsetosa\t2\n6.5\t2.8\tversicolor\t1\n6.6\t2.9\tversicolor\t1\n5.6\t3.0\tversicolor\t1\n6.7\t3.0\tversicolor\t1\n6.0\t2.9\tversicolor\t1\n5.4\t3.0\tversicolor\t1\n5.5\t2.5\tversicolor\t1\n5.5\t2.6\tversicolor\t1\n6.1\t3.0\tversicolor\t1\n5.6\t2.7\tversicolor\t1\n5.7\t2.9\tversicolor\t1\n6.3\t3.3\tvirginica\t2\n6.5\t3.0\tvirginica\t2\n7.6\t3.0\tvirginica\t2\n6.7\t2.5\tvirginica\t2\n5.7\t2.5\tvirginica\t2\n6.9\t3.2\tvirginica\t2\n6.7\t3.3\tvirginica\t2\n7.2\t3.2\tvirginica\t1\n6.2\t2.8\tvirginica\t2\n6.1\t3.0\tvirginica\t2\n7.2\t3.0\tvirginica\t1\n7.9\t3.8\tvirginica\t1\n6.0\t3.0\tvirginica\t2\n6.9\t3.1\tvirginica\t2\n5.8\t2.7\tvirginica\t2\n6.2\t3.4\tvirginica\t2\n5.9\t3.0\tvirginica\t2\n","type":"TABLE"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":"[{\"raw\":{\"height\":300,\"lastColumns\":[],\"version\":1}}]","hideInIFrame":false,"selectedVisualization":"table","title":"Use the model to extract the features (in this case there are two)","message":["%python","","z.show(nmf_mod_details.transform(IRIS_TEST, ","                                 supplemental_cols = IRIS_TEST[:, ","                                                               ['Sepal_Length']], ","                                                               topN = 2).sort_values(by = ['Sepal_Length', ","                                                                                           'TOP_1', ","                                                                                           'TOP_1_VAL']))"],"enabled":true,"result":{"startTime":1737152704851,"interpreter":"python.low","endTime":1737152705836,"results":[{"message":"Sepal_Length\tTOP_1\tTOP_1_VAL\tTOP_2\tTOP_2_VAL\n4.4\t2.0\t0.4640412598647486\t1.0\t0.0\n4.4\t2.0\t0.48205070749975476\t1.0\t0.04551789626526008\n4.8\t2.0\t0.4751689824687339\t1.0\t0.08387423104029225\n4.8\t2.0\t0.5103715990146052\t1.0\t0.10187994337557373\n4.9\t2.0\t0.4054515638134286\t1.0\t0.08463053258080423\n5.0\t2.0\t0.44268094272018427\t1.0\t0.11952411722275179\n5.0\t2.0\t0.448051484516526\t1.0\t0.10663485080969141\n5.1\t2.0\t0.46702754297091664\t1.0\t0.1842263710849203\n5.2\t2.0\t0.4412505995275515\t1.0\t0.17847013228958625\n5.2\t2.0\t0.45368430229013673\t1.0\t0.18430527012309156\n5.4\t1.0\t0.6499526498070691\t2.0\t0.5825217627358026\n5.4\t2.0\t0.506702757504397\t1.0\t0.19897675269807696\n5.5\t1.0\t0.6663969384059616\t2.0\t0.4314427691076418\n5.5\t1.0\t0.6883980842021493\t2.0\t0.4339864468719251\n5.6\t1.0\t0.6441707999471009\t2.0\t0.5538353891251782\n5.6\t1.0\t0.7065840726377054\t2.0\t0.45496962012396946\n5.7\t1.0\t0.7430035656071692\t2.0\t0.4564416223827207\n5.7\t2.0\t0.9057475835340096\t1.0\t0.5875678025287024\n5.8\t2.0\t0.898029318555704\t1.0\t0.6913492833143242\n5.9\t2.0\t0.8660619462488786\t1.0\t0.7618645042396146\n6.0\t1.0\t0.7179716673379009\t2.0\t0.5320325991132174\n6.0\t2.0\t0.8206681476035247\t1.0\t0.7582295835872603\n6.1\t1.0\t0.7814865812553904\t2.0\t0.5078737621583218\n6.1\t2.0\t0.8145080593621481\t1.0\t0.7503520266489467\n6.2\t2.0\t0.7899818817438358\t1.0\t0.7465080857909812\n6.2\t2.0\t1.0920418890298853\t1.0\t0.7593724727062061\n6.3\t2.0\t1.1832986856044607\t1.0\t0.63385831322487\n6.5\t1.0\t0.7348203160798916\t2.0\t0.4837531078966465\n6.5\t2.0\t1.0197383466441023\t1.0\t0.6930731704138097\n6.6\t1.0\t0.797151023093405\t2.0\t0.40153122583564926\n6.7\t1.0\t0.7921518174053821\t2.0\t0.616376842986629\n6.7\t2.0\t0.8279783466871486\t1.0\t0.7478711549532522\n6.7\t2.0\t0.981497979338616\t1.0\t0.8049438277858421\n6.9\t2.0\t0.9433283726762365\t1.0\t0.8494049418513676\n6.9\t2.0\t1.042632242983969\t1.0\t0.7869409806297116\n7.2\t1.0\t0.9150118568162859\t2.0\t0.8503300457722371\n7.2\t1.0\t0.9381115785667398\t2.0\t0.7452066956074388\n7.6\t2.0\t0.9807565276082126\t1.0\t0.8645083826943603\n7.9\t1.0\t1.0482874881012707\t2.0\t0.9477442120749615\n","type":"TABLE"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":null,"message":["%md\r","\r","#### Feature Compare\r","\r","Use `feature_compare(x, compare_cols=None, supplemental_cols=None)` to compare features of data and generate relatedness.\r","\r","**Parameters**\r","* `x` [an OML object] The data used to measure relatedness.\r","* `compare_cols` [str, a list of str or None (default)] The column(s) used to measure data relatedness. If None, all the columns of `x` are compared to measure relatedness.\r","* `supplemental_cols` [a list of str or None (default)] A list of columns to display along with the resulting \u2018SIMILARITY\u2019 column.\r","\r","**Returns**\r","* `pred` Contains a \u2018SIMILARITY\u2019 column that measures relatedness and supplementary columns if specified.\r"],"enabled":true,"result":{"startTime":1737152706307,"interpreter":"md.low","endTime":1737152706749,"results":[{"message":"<h4 id=\"feature-compare\">Feature Compare<\/h4>\n<p>Use <code>feature_compare(x, compare_cols=None, supplemental_cols=None)<\/code> to compare features of data and generate relatedness.<\/p>\n<p><strong>Parameters<\/strong><\/p>\n<ul>\n<li><code>x<\/code> [an OML object] The data used to measure relatedness.<\/li>\n<li><code>compare_cols<\/code> [str, a list of str or None (default)] The column(s) used to measure data relatedness. If None, all the columns of <code>x<\/code> are compared to measure relatedness.<\/li>\n<li><code>supplemental_cols<\/code> [a list of str or None (default)] A list of columns to display along with the resulting \u2018SIMILARITY\u2019 column.<\/li>\n<\/ul>\n<p><strong>Returns<\/strong><\/p>\n<ul>\n<li><code>pred<\/code> Contains a \u2018SIMILARITY\u2019 column that measures relatedness and supplementary columns if specified.<\/li>\n<\/ul>\n","type":"HTML"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":true,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":false,"hideVizConfig":true,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"table","title":"Compare features","message":["%python","","z.show(nmf_mod_details.feature_compare(IRIS_TEST, compare_cols = \"Sepal_Length\",","    supplemental_cols = [\"Species\"]))"],"enabled":true,"result":{"startTime":1737152710776,"interpreter":"python.low","endTime":1737152711359,"results":[{"message":"Species_A\tSpecies_B\tSIMILARITY\nsetosa\tsetosa\t1.0\nsetosa\tsetosa\t0.9466506042923339\nsetosa\tsetosa\t0.9836372821804514\nsetosa\tsetosa\t0.9672745643609025\nsetosa\tsetosa\t0.9345491287218048\nsetosa\tsetosa\t0.9672745643609024\nsetosa\tsetosa\t0.9672745643609024\nsetosa\tsetosa\t0.9466506042923339\nsetosa\tsetosa\t0.9836372821804511\nsetosa\tsetosa\t0.9672745643609025\nsetosa\tversicolor\t0.7545592327067687\nsetosa\tversicolor\t0.7381965148872198\nsetosa\tversicolor\t0.9018236930827074\nsetosa\tversicolor\t0.7218337970676711\nsetosa\tversicolor\t0.8363728218045123\nsetosa\tversicolor\t0.9345491287218048\nsetosa\tversicolor\t0.9181864109022562\nsetosa\tversicolor\t0.9181864109022562\nsetosa\tversicolor\t0.8200101039849635\nsetosa\tversicolor\t0.9018236930827074\nsetosa\tversicolor\t0.8854609752631587\nsetosa\tvirginica\t0.7872846683458663\nsetosa\tvirginica\t0.7545592327067687\nsetosa\tvirginica\t0.5745693366917324\nsetosa\tvirginica\t0.7218337970676711\nsetosa\tvirginica\t0.8854609752631587\nsetosa\tvirginica\t0.6891083614285737\nsetosa\tvirginica\t0.7218337970676711\nsetosa\tvirginica\t0.6400202079699274\nsetosa\tvirginica\t0.803647386165415\nsetosa\tvirginica\t0.8200101039849635\nsetosa\tvirginica\t0.6400202079699274\nsetosa\tvirginica\t0.5254811832330861\nsetosa\tvirginica\t0.8363728218045123\nsetosa\tvirginica\t0.6891083614285737\nsetosa\tvirginica\t0.8690982574436099\nsetosa\tvirginica\t0.803647386165415\nsetosa\tvirginica\t0.852735539624061\nsetosa\tsetosa\t0.9466506042923339\nsetosa\tsetosa\t0.9836372821804514\nsetosa\tsetosa\t0.9672745643609025\nsetosa\tsetosa\t0.9345491287218048\nsetosa\tsetosa\t0.9672745643609024\nsetosa\tsetosa\t0.9672745643609024\nsetosa\tsetosa\t0.9466506042923339\nsetosa\tsetosa\t0.9836372821804511\nsetosa\tsetosa\t0.9672745643609025\nsetosa\tversicolor\t0.7545592327067687\nsetosa\tversicolor\t0.7381965148872198\nsetosa\tversicolor\t0.9018236930827074\nsetosa\tversicolor\t0.7218337970676711\nsetosa\tversicolor\t0.8363728218045123\nsetosa\tversicolor\t0.9345491287218048\nsetosa\tversicolor\t0.9181864109022562\nsetosa\tversicolor\t0.9181864109022562\nsetosa\tversicolor\t0.8200101039849635\nsetosa\tversicolor\t0.9018236930827074\nsetosa\tversicolor\t0.8854609752631587\nsetosa\tvirginica\t0.7872846683458663\nsetosa\tvirginica\t0.7545592327067687\nsetosa\tvirginica\t0.5745693366917324\nsetosa\tvirginica\t0.7218337970676711\nsetosa\tvirginica\t0.8854609752631587\nsetosa\tvirginica\t0.6891083614285737\nsetosa\tvirginica\t0.7218337970676711\nsetosa\tvirginica\t0.6400202079699274\nsetosa\tvirginica\t0.803647386165415\nsetosa\tvirginica\t0.8200101039849635\nsetosa\tvirginica\t0.6400202079699274\nsetosa\tvirginica\t0.5254811832330861\nsetosa\tvirginica\t0.8363728218045123\nsetosa\tvirginica\t0.6891083614285737\nsetosa\tvirginica\t0.8690982574436099\nsetosa\tvirginica\t0.803647386165415\nsetosa\tvirginica\t0.852735539624061\nsetosa\tsetosa\t0.960560382997515\nsetosa\tsetosa\t0.9717106764818138\nsetosa\tsetosa\t0.884398844108294\nsetosa\tsetosa\t0.9161314212880228\nsetosa\tsetosa\t0.9161314212880228\nsetosa\tsetosa\t1.0\nsetosa\tsetosa\t0.931645704419342\nsetosa\tsetosa\t0.9717106764818138\nsetosa\tversicolor\t0.705953191167541\nsetosa\tversicolor\t0.6896424102430023\nsetosa\tversicolor\t0.852232533122587\nsetosa\tversicolor\t0.6733264346111284\nsetosa\tversicolor\t0.7873873401188978\nsetosa\tversicolor\t0.884398844108294\nsetosa\tversicolor\t0.8683501339879499\nsetosa\tversicolor\t0.8683501339879499\nsetosa\tversicolor\t0.7711221602415042\nsetosa\tversicolor\t0.852232533122587\nsetosa\tversicolor\t0.8360663598201141\nsetosa\tvirginica\t0.7385552801818989\nsetosa\tvirginica\t0.705953191167541\nsetosa\tvirginica\t0.5263375150377496\nsetosa\tvirginica\t0.6733264346111284\nsetosa\tvirginica\t0.8360663598201141\nsetosa\tvirginica\t0.6406817300237304\nsetosa\tvirginica\t0.6733264346111284\nsetosa\tvirginica\t0.5916904380526152\nsetosa\tvirginica\t0.754843998661672\nsetosa\tvirginica\t0.7711221602415042\nsetosa\tvirginica\t0.5916904380526152\nsetosa\tvirginica\t0.4773067368464452\nsetosa\tvirginica\t0.7873873401188978\nsetosa\tvirginica\t0.6406817300237304\nsetosa\tvirginica\t0.8198646909493489\nsetosa\tvirginica\t0.754843998661672\nsetosa\tvirginica\t0.8036363123696153\nsetosa\tsetosa\t0.9836372821804511\nsetosa\tsetosa\t0.9181864109022563\nsetosa\tsetosa\t0.9509118465413539\nsetosa\tsetosa\t0.9509118465413539\nsetosa\tsetosa\t0.960560382997515\nsetosa\tsetosa\t0.9672745643609025\nsetosa\tsetosa\t0.9836372821804511\nsetosa\tversicolor\t0.73819651488722\nsetosa\tversicolor\t0.7218337970676714\nsetosa\tversicolor\t0.8854609752631587\nsetosa\tversicolor\t0.7054710792481225\nsetosa\tversicolor\t0.8200101039849638\nsetosa\tversicolor\t0.9181864109022563\nsetosa\tversicolor\t0.9018236930827076\nsetosa\tversicolor\t0.9018236930827076\nsetosa\tversicolor\t0.803647386165415\nsetosa\tversicolor\t0.8854609752631587\nsetosa\tversicolor\t0.86909825744361\nsetosa\tvirginica\t0.7709219505263176\nsetosa\tvirginica\t0.73819651488722\nsetosa\tvirginica\t0.5582066188721837\nsetosa\tvirginica\t0.7054710792481225\nsetosa\tvirginica\t0.86909825744361\nsetosa\tvirginica\t0.6727456436090251\nsetosa\tvirginica\t0.7054710792481225\nsetosa\tvirginica\t0.6236574901503789\nsetosa\tvirginica\t0.7872846683458663\nsetosa\tvirginica\t0.803647386165415\nsetosa\tvirginica\t0.6236574901503789\nsetosa\tvirginica\t0.5091184654135374\nsetosa\tvirginica\t0.8200101039849638\nsetosa\tvirginica\t0.6727456436090251\nsetosa\tvirginica\t0.8527355396240612\nsetosa\tvirginica\t0.7872846683458663\nsetosa\tvirginica\t0.8363728218045124\nsetosa\tsetosa\t0.9018236930827075\nsetosa\tsetosa\t0.9345491287218051\nsetosa\tsetosa\t0.9345491287218051\nsetosa\tsetosa\t0.9717106764818138\nsetosa\tsetosa\t0.9509118465413537\nsetosa\tsetosa\t1.0\nsetosa\tversicolor\t0.7218337970676711\nsetosa\tversicolor\t0.7054710792481225\nsetosa\tversicolor\t0.8690982574436099\nsetosa\tversicolor\t0.6891083614285737\nsetosa\tversicolor\t0.803647386165415\nsetosa\tversicolor\t0.9018236930827075\nsetosa\tversicolor\t0.8854609752631587\nsetosa\tversicolor\t0.8854609752631587\nsetosa\tversicolor\t0.7872846683458661\nsetosa\tversicolor\t0.8690982574436099\nsetosa\tversicolor\t0.8527355396240612\nsetosa\tvirginica\t0.7545592327067687\nsetosa\tvirginica\t0.7218337970676711\nsetosa\tvirginica\t0.5418439010526348\nsetosa\tvirginica\t0.6891083614285737\nsetosa\tvirginica\t0.8527355396240612\nsetosa\tvirginica\t0.6563829257894762\nsetosa\tvirginica\t0.6891083614285737\nsetosa\tvirginica\t0.60729477233083\nsetosa\tvirginica\t0.7709219505263175\nsetosa\tvirginica\t0.7872846683458661\nsetosa\tvirginica\t0.60729477233083\nsetosa\tvirginica\t0.49275574759398866\nsetosa\tvirginica\t0.803647386165415\nsetosa\tvirginica\t0.6563829257894762\nsetosa\tvirginica\t0.8363728218045124\nsetosa\tvirginica\t0.7709219505263175\nsetosa\tvirginica\t0.8200101039849635\nsetosa\tsetosa\t0.9672745643609024\nsetosa\tsetosa\t0.9672745643609024\nsetosa\tsetosa\t0.884398844108294\nsetosa\tsetosa\t0.9509118465413537\nsetosa\tsetosa\t0.9018236930827075\nsetosa\tversicolor\t0.8200101039849637\nsetosa\tversicolor\t0.803647386165415\nsetosa\tversicolor\t0.9672745643609024\nsetosa\tversicolor\t0.7872846683458662\nsetosa\tversicolor\t0.9018236930827075\nsetosa\tversicolor\t1.0\nsetosa\tversicolor\t0.9836372821804513\nsetosa\tversicolor\t0.9836372821804513\nsetosa\tversicolor\t0.8854609752631586\nsetosa\tversicolor\t0.9672745643609024\nsetosa\tversicolor\t0.9509118465413537\nsetosa\tvirginica\t0.8527355396240613\nsetosa\tvirginica\t0.8200101039849637\nsetosa\tvirginica\t0.6400202079699274\nsetosa\tvirginica\t0.7872846683458662\nsetosa\tvirginica\t0.9509118465413537\nsetosa\tvirginica\t0.7545592327067687\nsetosa\tvirginica\t0.7872846683458662\nsetosa\tvirginica\t0.7054710792481225\nsetosa\tvirginica\t0.86909825744361\nsetosa\tvirginica\t0.8854609752631586\nsetosa\tvirginica\t0.7054710792481225\nsetosa\tvirginica\t0.5909320545112812\nsetosa\tvirginica\t0.9018236930827075\nsetosa\tvirginica\t0.7545592327067687\nsetosa\tvirginica\t0.934549128721805\nsetosa\tvirginica\t0.86909825744361\nsetosa\tvirginica\t0.9181864109022562\nsetosa\tsetosa\t1.0\nsetosa\tsetosa\t0.9161314212880228\nsetosa\tsetosa\t0.9836372821804513\nsetosa\tsetosa\t0.9345491287218051\nsetosa\tversicolor\t0.7872846683458662\nsetosa\tversicolor\t0.7709219505263174\nsetosa\tversicolor\t0.9345491287218048\nsetosa\tversicolor\t0.7545592327067687\nsetosa\tversicolor\t0.8690982574436099\nsetosa\tversicolor\t0.9672745643609024\nsetosa\tversicolor\t0.9509118465413537\nsetosa\tversicolor\t0.9509118465413537\nsetosa\tversicolor\t0.852735539624061\nsetosa\tversicolor\t0.9345491287218048\nsetosa\tversicolor\t0.9181864109022562\nsetosa\tvirginica\t0.8200101039849638\nsetosa\tvirginica\t0.7872846683458662\nsetosa\tvirginica\t0.6072947723308298\nsetosa\tvirginica\t0.7545592327067687\nsetosa\tvirginica\t0.9181864109022562\nsetosa\tvirginica\t0.7218337970676711\nsetosa\tvirginica\t0.7545592327067687\nsetosa\tvirginica\t0.6727456436090249\nsetosa\tvirginica\t0.8363728218045124\nsetosa\tvirginica\t0.852735539624061\nsetosa\tvirginica\t0.6727456436090249\nsetosa\tvirginica\t0.5582066188721837\nsetosa\tvirginica\t0.8690982574436099\nsetosa\tvirginica\t0.7218337970676711\nsetosa\tvirginica\t0.9018236930827075\nsetosa\tvirginica\t0.8363728218045124\nsetosa\tvirginica\t0.8854609752631586\nsetosa\tsetosa\t0.9161314212880228\nsetosa\tsetosa\t0.9836372821804513\nsetosa\tsetosa\t0.9345491287218051\nsetosa\tversicolor\t0.7872846683458662\nsetosa\tversicolor\t0.7709219505263174\nsetosa\tversicolor\t0.9345491287218048\nsetosa\tversicolor\t0.7545592327067687\nsetosa\tversicolor\t0.8690982574436099\nsetosa\tversicolor\t0.9672745643609024\nsetosa\tversicolor\t0.9509118465413537\nsetosa\tversicolor\t0.9509118465413537\nsetosa\tversicolor\t0.852735539624061\nsetosa\tversicolor\t0.9345491287218048\nsetosa\tversicolor\t0.9181864109022562\nsetosa\tvirginica\t0.8200101039849638\nsetosa\tvirginica\t0.7872846683458662\nsetosa\tvirginica\t0.6072947723308298\nsetosa\tvirginica\t0.7545592327067687\nsetosa\tvirginica\t0.9181864109022562\nsetosa\tvirginica\t0.7218337970676711\nsetosa\tvirginica\t0.7545592327067687\nsetosa\tvirginica\t0.6727456436090249\nsetosa\tvirginica\t0.8363728218045124\nsetosa\tvirginica\t0.852735539624061\nsetosa\tvirginica\t0.6727456436090249\nsetosa\tvirginica\t0.5582066188721837\nsetosa\tvirginica\t0.8690982574436099\nsetosa\tvirginica\t0.7218337970676711\nsetosa\tvirginica\t0.9018236930827075\nsetosa\tvirginica\t0.8363728218045124\nsetosa\tvirginica\t0.8854609752631586\nsetosa\tsetosa\t0.931645704419342\nsetosa\tsetosa\t0.9717106764818138\nsetosa\tversicolor\t0.705953191167541\nsetosa\tversicolor\t0.6896424102430023\nsetosa\tversicolor\t0.852232533122587\nsetosa\tversicolor\t0.6733264346111284\nsetosa\tversicolor\t0.7873873401188978\nsetosa\tversicolor\t0.884398844108294\nsetosa\tversicolor\t0.8683501339879499\nsetosa\tversicolor\t0.8683501339879499\nsetosa\tversicolor\t0.7711221602415042\nsetosa\tversicolor\t0.852232533122587\nsetosa\tversicolor\t0.8360663598201141\nsetosa\tvirginica\t0.7385552801818989\nsetosa\tvirginica\t0.705953191167541\nsetosa\tvirginica\t0.5263375150377496\nsetosa\tvirginica\t0.6733264346111284\nsetosa\tvirginica\t0.8360663598201141\nsetosa\tvirginica\t0.6406817300237304\nsetosa\tvirginica\t0.6733264346111284\nsetosa\tvirginica\t0.5916904380526152\nsetosa\tvirginica\t0.754843998661672\nsetosa\tvirginica\t0.7711221602415042\nsetosa\tvirginica\t0.5916904380526152\nsetosa\tvirginica\t0.4773067368464452\nsetosa\tvirginica\t0.7873873401188978\nsetosa\tvirginica\t0.6406817300237304\nsetosa\tvirginica\t0.8198646909493489\nsetosa\tvirginica\t0.754843998661672\nsetosa\tvirginica\t0.8036363123696153\nsetosa\tsetosa\t0.9509118465413537\nsetosa\tversicolor\t0.7709219505263175\nsetosa\tversicolor\t0.7545592327067687\nsetosa\tversicolor\t0.9181864109022562\nsetosa\tversicolor\t0.73819651488722\nsetosa\tversicolor\t0.8527355396240612\nsetosa\tversicolor\t0.9509118465413537\nsetosa\tversicolor\t0.9345491287218051\nsetosa\tversicolor\t0.9345491287218051\nsetosa\tversicolor\t0.8363728218045123\nsetosa\tversicolor\t0.9181864109022562\nsetosa\tversicolor\t0.9018236930827075\nsetosa\tvirginica\t0.8036473861654151\nsetosa\tvirginica\t0.7709219505263175\nsetosa\tvirginica\t0.5909320545112812\nsetosa\tvirginica\t0.73819651488722\nsetosa\tvirginica\t0.9018236930827075\nsetosa\tvirginica\t0.7054710792481225\nsetosa\tvirginica\t0.73819651488722\nsetosa\tvirginica\t0.6563829257894763\nsetosa\tvirginica\t0.8200101039849639\nsetosa\tvirginica\t0.8363728218045123\nsetosa\tvirginica\t0.6563829257894763\nsetosa\tvirginica\t0.541843901052635\nsetosa\tvirginica\t0.8527355396240612\nsetosa\tvirginica\t0.7054710792481225\nsetosa\tvirginica\t0.8854609752631587\nsetosa\tvirginica\t0.8200101039849639\nsetosa\tvirginica\t0.8690982574436099\nsetosa\tversicolor\t0.7218337970676711\nsetosa\tversicolor\t0.7054710792481225\nsetosa\tversicolor\t0.8690982574436099\nsetosa\tversicolor\t0.6891083614285737\nsetosa\tversicolor\t0.803647386165415\nsetosa\tversicolor\t0.9018236930827075\nsetosa\tversicolor\t0.8854609752631587\nsetosa\tversicolor\t0.8854609752631587\nsetosa\tversicolor\t0.7872846683458661\nsetosa\tversicolor\t0.8690982574436099\nsetosa\tversicolor\t0.8527355396240612\nsetosa\tvirginica\t0.7545592327067687\nsetosa\tvirginica\t0.7218337970676711\nsetosa\tvirginica\t0.5418439010526348\nsetosa\tvirginica\t0.6891083614285737\nsetosa\tvirginica\t0.8527355396240612\nsetosa\tvirginica\t0.6563829257894762\nsetosa\tvirginica\t0.6891083614285737\nsetosa\tvirginica\t0.60729477233083\nsetosa\tvirginica\t0.7709219505263175\nsetosa\tvirginica\t0.7872846683458661\nsetosa\tvirginica\t0.60729477233083\nsetosa\tvirginica\t0.49275574759398866\nsetosa\tvirginica\t0.803647386165415\nsetosa\tvirginica\t0.6563829257894762\nsetosa\tvirginica\t0.8363728218045124\nsetosa\tvirginica\t0.7709219505263175\nsetosa\tvirginica\t0.8200101039849635\nversicolor\tversicolor\t0.9836372821804512\nversicolor\tversicolor\t0.8527355396240612\nversicolor\tversicolor\t0.9672745643609025\nversicolor\tversicolor\t0.9181864109022563\nversicolor\tversicolor\t0.8200101039849637\nversicolor\tversicolor\t0.8363728218045126\nversicolor\tversicolor\t0.8363728218045126\nversicolor\tversicolor\t0.9345491287218051\nversicolor\tversicolor\t0.8527355396240612\nversicolor\tversicolor\t0.86909825744361\nversicolor\tvirginica\t0.9672745643609024\nversicolor\tvirginica\t1.0\nversicolor\tvirginica\t0.8200101039849637\nversicolor\tvirginica\t0.9672745643609025\nversicolor\tvirginica\t0.86909825744361\nversicolor\tvirginica\t0.9345491287218051\nversicolor\tvirginica\t0.9672745643609025\nversicolor\tvirginica\t0.8854609752631588\nversicolor\tvirginica\t0.9509118465413536\nversicolor\tvirginica\t0.9345491287218051\nversicolor\tvirginica\t0.8854609752631588\nversicolor\tvirginica\t0.7709219505263174\nversicolor\tvirginica\t0.9181864109022563\nversicolor\tvirginica\t0.9345491287218051\nversicolor\tvirginica\t0.8854609752631588\nversicolor\tvirginica\t0.9509118465413536\nversicolor\tvirginica\t0.9018236930827076\nversicolor\tversicolor\t0.8363728218045126\nversicolor\tversicolor\t0.9836372821804512\nversicolor\tversicolor\t0.9018236930827075\nversicolor\tversicolor\t0.803647386165415\nversicolor\tversicolor\t0.8200101039849637\nversicolor\tversicolor\t0.8200101039849637\nversicolor\tversicolor\t0.9181864109022564\nversicolor\tversicolor\t0.8363728218045126\nversicolor\tversicolor\t0.8527355396240612\nversicolor\tvirginica\t0.9509118465413536\nversicolor\tvirginica\t0.9836372821804512\nversicolor\tvirginica\t0.8363728218045124\nversicolor\tvirginica\t0.9836372821804512\nversicolor\tvirginica\t0.8527355396240612\nversicolor\tvirginica\t0.9509118465413537\nversicolor\tvirginica\t0.9836372821804512\nversicolor\tvirginica\t0.9018236930827076\nversicolor\tvirginica\t0.934549128721805\nversicolor\tvirginica\t0.9181864109022564\nversicolor\tvirginica\t0.9018236930827076\nversicolor\tvirginica\t0.7872846683458662\nversicolor\tvirginica\t0.9018236930827075\nversicolor\tvirginica\t0.9509118465413537\nversicolor\tvirginica\t0.86909825744361\nversicolor\tvirginica\t0.934549128721805\nversicolor\tvirginica\t0.8854609752631588\nversicolor\tversicolor\t0.8200101039849638\nversicolor\tversicolor\t0.9345491287218051\nversicolor\tversicolor\t0.9672745643609024\nversicolor\tversicolor\t0.9836372821804512\nversicolor\tversicolor\t0.9836372821804512\nversicolor\tversicolor\t0.9181864109022562\nversicolor\tversicolor\t1.0\nversicolor\tversicolor\t0.9836372821804513\nversicolor\tvirginica\t0.8854609752631589\nversicolor\tvirginica\t0.8527355396240612\nversicolor\tvirginica\t0.6727456436090249\nversicolor\tvirginica\t0.8200101039849638\nversicolor\tvirginica\t0.9836372821804513\nversicolor\tvirginica\t0.7872846683458663\nversicolor\tvirginica\t0.8200101039849638\nversicolor\tvirginica\t0.73819651488722\nversicolor\tvirginica\t0.9018236930827076\nversicolor\tvirginica\t0.9181864109022562\nversicolor\tvirginica\t0.73819651488722\nversicolor\tvirginica\t0.6236574901503787\nversicolor\tvirginica\t0.9345491287218051\nversicolor\tvirginica\t0.7872846683458663\nversicolor\tvirginica\t0.9672745643609025\nversicolor\tvirginica\t0.9018236930827076\nversicolor\tvirginica\t0.9509118465413537\nversicolor\tversicolor\t0.8854609752631588\nversicolor\tversicolor\t0.7872846683458662\nversicolor\tversicolor\t0.803647386165415\nversicolor\tversicolor\t0.803647386165415\nversicolor\tversicolor\t0.9018236930827076\nversicolor\tversicolor\t0.8200101039849638\nversicolor\tversicolor\t0.8363728218045126\nversicolor\tvirginica\t0.9345491287218048\nversicolor\tvirginica\t0.9672745643609025\nversicolor\tvirginica\t0.8527355396240612\nversicolor\tvirginica\t1.0\nversicolor\tvirginica\t0.8363728218045126\nversicolor\tvirginica\t0.9672745643609025\nversicolor\tvirginica\t1.0\nversicolor\tvirginica\t0.9181864109022563\nversicolor\tvirginica\t0.9181864109022562\nversicolor\tvirginica\t0.9018236930827076\nversicolor\tvirginica\t0.9181864109022563\nversicolor\tvirginica\t0.803647386165415\nversicolor\tvirginica\t0.8854609752631588\nversicolor\tvirginica\t0.9672745643609025\nversicolor\tvirginica\t0.8527355396240612\nversicolor\tvirginica\t0.9181864109022562\nversicolor\tvirginica\t0.8690982574436101\nversicolor\tversicolor\t0.9018236930827075\nversicolor\tversicolor\t0.9181864109022562\nversicolor\tversicolor\t0.9181864109022562\nversicolor\tversicolor\t0.9836372821804511\nversicolor\tversicolor\t0.9345491287218051\nversicolor\tversicolor\t0.9509118465413537\nversicolor\tvirginica\t0.9509118465413539\nversicolor\tvirginica\t0.9181864109022563\nversicolor\tvirginica\t0.7381965148872199\nversicolor\tvirginica\t0.8854609752631588\nversicolor\tvirginica\t0.9509118465413537\nversicolor\tvirginica\t0.8527355396240612\nversicolor\tvirginica\t0.8854609752631588\nversicolor\tvirginica\t0.8036473861654151\nversicolor\tvirginica\t0.9672745643609025\nversicolor\tvirginica\t0.9836372821804511\nversicolor\tvirginica\t0.8036473861654151\nversicolor\tvirginica\t0.6891083614285737\nversicolor\tvirginica\t1.0\nversicolor\tvirginica\t0.8527355396240612\nversicolor\tvirginica\t0.9672745643609025\nversicolor\tvirginica\t0.9672745643609025\nversicolor\tvirginica\t0.9836372821804513\nversicolor\tversicolor\t0.9836372821804513\nversicolor\tversicolor\t0.9836372821804513\nversicolor\tversicolor\t0.8854609752631586\nversicolor\tversicolor\t0.9672745643609024\nversicolor\tversicolor\t0.9509118465413537\nversicolor\tvirginica\t0.8527355396240613\nversicolor\tvirginica\t0.8200101039849637\nversicolor\tvirginica\t0.6400202079699274\nversicolor\tvirginica\t0.7872846683458662\nversicolor\tvirginica\t0.9509118465413537\nversicolor\tvirginica\t0.7545592327067687\nversicolor\tvirginica\t0.7872846683458662\nversicolor\tvirginica\t0.7054710792481225\nversicolor\tvirginica\t0.86909825744361\nversicolor\tvirginica\t0.8854609752631586\nversicolor\tvirginica\t0.7054710792481225\nversicolor\tvirginica\t0.5909320545112812\nversicolor\tvirginica\t0.9018236930827075\nversicolor\tvirginica\t0.7545592327067687\nversicolor\tvirginica\t0.934549128721805\nversicolor\tvirginica\t0.86909825744361\nversicolor\tvirginica\t0.9181864109022562\nversicolor\tversicolor\t1.0\nversicolor\tversicolor\t0.9018236930827074\nversicolor\tversicolor\t0.9836372821804512\nversicolor\tversicolor\t0.9672745643609024\nversicolor\tvirginica\t0.8690982574436101\nversicolor\tvirginica\t0.8363728218045126\nversicolor\tvirginica\t0.6563829257894762\nversicolor\tvirginica\t0.803647386165415\nversicolor\tvirginica\t0.9672745643609024\nversicolor\tvirginica\t0.7709219505263175\nversicolor\tvirginica\t0.803647386165415\nversicolor\tvirginica\t0.7218337970676714\nversicolor\tvirginica\t0.8854609752631588\nversicolor\tvirginica\t0.9018236930827074\nversicolor\tvirginica\t0.7218337970676714\nversicolor\tvirginica\t0.60729477233083\nversicolor\tvirginica\t0.9181864109022562\nversicolor\tvirginica\t0.7709219505263175\nversicolor\tvirginica\t0.9509118465413537\nversicolor\tvirginica\t0.8854609752631588\nversicolor\tvirginica\t0.9345491287218048\nversicolor\tversicolor\t0.9018236930827074\nversicolor\tversicolor\t0.9836372821804512\nversicolor\tversicolor\t0.9672745643609024\nversicolor\tvirginica\t0.8690982574436101\nversicolor\tvirginica\t0.8363728218045126\nversicolor\tvirginica\t0.6563829257894762\nversicolor\tvirginica\t0.803647386165415\nversicolor\tvirginica\t0.9672745643609024\nversicolor\tvirginica\t0.7709219505263175\nversicolor\tvirginica\t0.803647386165415\nversicolor\tvirginica\t0.7218337970676714\nversicolor\tvirginica\t0.8854609752631588\nversicolor\tvirginica\t0.9018236930827074\nversicolor\tvirginica\t0.7218337970676714\nversicolor\tvirginica\t0.60729477233083\nversicolor\tvirginica\t0.9181864109022562\nversicolor\tvirginica\t0.7709219505263175\nversicolor\tvirginica\t0.9509118465413537\nversicolor\tvirginica\t0.8854609752631588\nversicolor\tvirginica\t0.9345491287218048\nversicolor\tversicolor\t0.9181864109022562\nversicolor\tversicolor\t0.9345491287218048\nversicolor\tvirginica\t0.9672745643609028\nversicolor\tvirginica\t0.9345491287218051\nversicolor\tvirginica\t0.7545592327067687\nversicolor\tvirginica\t0.9018236930827076\nversicolor\tvirginica\t0.9345491287218048\nversicolor\tvirginica\t0.8690982574436101\nversicolor\tvirginica\t0.9018236930827076\nversicolor\tvirginica\t0.8200101039849639\nversicolor\tvirginica\t0.9836372821804514\nversicolor\tvirginica\t1.0\nversicolor\tvirginica\t0.8200101039849639\nversicolor\tvirginica\t0.7054710792481226\nversicolor\tvirginica\t0.9836372821804511\nversicolor\tvirginica\t0.8690982574436101\nversicolor\tvirginica\t0.9509118465413536\nversicolor\tvirginica\t0.9836372821804514\nversicolor\tvirginica\t0.9672745643609025\nversicolor\tversicolor\t0.9836372821804513\nversicolor\tvirginica\t0.8854609752631589\nversicolor\tvirginica\t0.8527355396240612\nversicolor\tvirginica\t0.6727456436090249\nversicolor\tvirginica\t0.8200101039849638\nversicolor\tvirginica\t0.9836372821804513\nversicolor\tvirginica\t0.7872846683458663\nversicolor\tvirginica\t0.8200101039849638\nversicolor\tvirginica\t0.73819651488722\nversicolor\tvirginica\t0.9018236930827076\nversicolor\tvirginica\t0.9181864109022562\nversicolor\tvirginica\t0.73819651488722\nversicolor\tvirginica\t0.6236574901503787\nversicolor\tvirginica\t0.9345491287218051\nversicolor\tvirginica\t0.7872846683458663\nversicolor\tvirginica\t0.9672745643609025\nversicolor\tvirginica\t0.9018236930827076\nversicolor\tvirginica\t0.9509118465413537\nversicolor\tvirginica\t0.9018236930827076\nversicolor\tvirginica\t0.86909825744361\nversicolor\tvirginica\t0.6891083614285737\nversicolor\tvirginica\t0.8363728218045126\nversicolor\tvirginica\t1.0\nversicolor\tvirginica\t0.803647386165415\nversicolor\tvirginica\t0.8363728218045126\nversicolor\tvirginica\t0.7545592327067687\nversicolor\tvirginica\t0.9181864109022563\nversicolor\tvirginica\t0.9345491287218048\nversicolor\tvirginica\t0.7545592327067687\nversicolor\tvirginica\t0.6400202079699274\nversicolor\tvirginica\t0.9509118465413537\nversicolor\tvirginica\t0.803647386165415\nversicolor\tvirginica\t0.9836372821804512\nversicolor\tvirginica\t0.9181864109022563\nversicolor\tvirginica\t0.9672745643609024\nvirginica\tvirginica\t0.9672745643609024\nvirginica\tvirginica\t0.7872846683458661\nvirginica\tvirginica\t0.9345491287218048\nvirginica\tvirginica\t0.9018236930827076\nvirginica\tvirginica\t0.9018236930827074\nvirginica\tvirginica\t0.9345491287218048\nvirginica\tvirginica\t0.8527355396240612\nvirginica\tvirginica\t0.9836372821804513\nvirginica\tvirginica\t0.9672745643609028\nvirginica\tvirginica\t0.8527355396240612\nvirginica\tvirginica\t0.7381965148872198\nvirginica\tvirginica\t0.9509118465413539\nvirginica\tvirginica\t0.9018236930827074\nvirginica\tvirginica\t0.9181864109022564\nvirginica\tvirginica\t0.9836372821804513\nvirginica\tvirginica\t0.9345491287218052\nvirginica\tvirginica\t0.8200101039849637\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t0.86909825744361\nvirginica\tvirginica\t0.9345491287218051\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t0.8854609752631588\nvirginica\tvirginica\t0.9509118465413536\nvirginica\tvirginica\t0.9345491287218051\nvirginica\tvirginica\t0.8854609752631588\nvirginica\tvirginica\t0.7709219505263174\nvirginica\tvirginica\t0.9181864109022563\nvirginica\tvirginica\t0.9345491287218051\nvirginica\tvirginica\t0.8854609752631588\nvirginica\tvirginica\t0.9509118465413536\nvirginica\tvirginica\t0.9018236930827076\nvirginica\tvirginica\t0.8527355396240612\nvirginica\tvirginica\t0.6891083614285737\nvirginica\tvirginica\t0.8854609752631587\nvirginica\tvirginica\t0.8527355396240612\nvirginica\tvirginica\t0.9345491287218048\nvirginica\tvirginica\t0.7709219505263174\nvirginica\tvirginica\t0.7545592327067687\nvirginica\tvirginica\t0.9345491287218048\nvirginica\tvirginica\t0.9509118465413537\nvirginica\tvirginica\t0.7381965148872199\nvirginica\tvirginica\t0.8854609752631587\nvirginica\tvirginica\t0.7054710792481225\nvirginica\tvirginica\t0.7709219505263174\nvirginica\tvirginica\t0.7218337970676714\nvirginica\tvirginica\t0.8363728218045126\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t1.0\nvirginica\tvirginica\t0.9181864109022563\nvirginica\tvirginica\t0.9181864109022562\nvirginica\tvirginica\t0.9018236930827076\nvirginica\tvirginica\t0.9181864109022563\nvirginica\tvirginica\t0.803647386165415\nvirginica\tvirginica\t0.8854609752631588\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t0.8527355396240612\nvirginica\tvirginica\t0.9181864109022562\nvirginica\tvirginica\t0.8690982574436101\nvirginica\tvirginica\t0.803647386165415\nvirginica\tvirginica\t0.8363728218045126\nvirginica\tvirginica\t0.7545592327067687\nvirginica\tvirginica\t0.9181864109022563\nvirginica\tvirginica\t0.9345491287218048\nvirginica\tvirginica\t0.7545592327067687\nvirginica\tvirginica\t0.6400202079699274\nvirginica\tvirginica\t0.9509118465413537\nvirginica\tvirginica\t0.803647386165415\nvirginica\tvirginica\t0.9836372821804512\nvirginica\tvirginica\t0.9181864109022563\nvirginica\tvirginica\t0.9672745643609024\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t0.9509118465413537\nvirginica\tvirginica\t0.8854609752631587\nvirginica\tvirginica\t0.8690982574436101\nvirginica\tvirginica\t0.9509118465413537\nvirginica\tvirginica\t0.8363728218045124\nvirginica\tvirginica\t0.8527355396240612\nvirginica\tvirginica\t1.0\nvirginica\tvirginica\t0.8200101039849638\nvirginica\tvirginica\t0.8854609752631587\nvirginica\tvirginica\t0.8363728218045127\nvirginica\tvirginica\t0.9181864109022563\nvirginica\tvirginica\t0.9181864109022562\nvirginica\tvirginica\t0.9018236930827076\nvirginica\tvirginica\t0.9181864109022563\nvirginica\tvirginica\t0.803647386165415\nvirginica\tvirginica\t0.8854609752631588\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t0.8527355396240612\nvirginica\tvirginica\t0.9181864109022562\nvirginica\tvirginica\t0.8690982574436101\nvirginica\tvirginica\t0.8363728218045126\nvirginica\tvirginica\t0.8200101039849639\nvirginica\tvirginica\t1.0\nvirginica\tvirginica\t0.8854609752631587\nvirginica\tvirginica\t0.8036473861654151\nvirginica\tvirginica\t0.9509118465413537\nvirginica\tvirginica\t0.7709219505263175\nvirginica\tvirginica\t0.8363728218045126\nvirginica\tvirginica\t0.7872846683458664\nvirginica\tvirginica\t0.9836372821804514\nvirginica\tvirginica\t0.8363728218045126\nvirginica\tvirginica\t0.7218337970676711\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t0.8854609752631587\nvirginica\tvirginica\t0.9345491287218051\nvirginica\tvirginica\t1.0\nvirginica\tvirginica\t0.950911846541354\nvirginica\tvirginica\t0.8200101039849639\nvirginica\tvirginica\t0.7054710792481226\nvirginica\tvirginica\t0.9836372821804511\nvirginica\tvirginica\t0.8690982574436101\nvirginica\tvirginica\t0.9509118465413536\nvirginica\tvirginica\t0.9836372821804514\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t0.8854609752631587\nvirginica\tvirginica\t0.8036473861654151\nvirginica\tvirginica\t0.9509118465413537\nvirginica\tvirginica\t0.7709219505263175\nvirginica\tvirginica\t0.8363728218045126\nvirginica\tvirginica\t0.7872846683458664\nvirginica\tvirginica\t0.6891083614285737\nvirginica\tvirginica\t0.8363728218045124\nvirginica\tvirginica\t0.6563829257894762\nvirginica\tvirginica\t0.7218337970676711\nvirginica\tvirginica\t0.6727456436090251\nvirginica\tvirginica\t0.8527355396240612\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t0.9836372821804513\nvirginica\tvirginica\t0.8200101039849638\nvirginica\tvirginica\t0.8854609752631587\nvirginica\tvirginica\t0.8363728218045127\nvirginica\tvirginica\t0.9345491287218051\nvirginica\tvirginica\t0.9836372821804511\nvirginica\tvirginica\t0.950911846541354\n","type":"TABLE"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":6,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"table","title":"Compare features","message":["%python","","z.show(nmf_mod_details.feature_compare(IRIS_TEST, compare_cols = [\"Sepal_Length\",","    \"Petal_Length\"], supplemental_cols = [\"Species\"]))"],"enabled":true,"result":{"startTime":1737152711896,"interpreter":"python.low","endTime":1737152712469,"results":[{"message":"Species_A\tSpecies_B\tSIMILARITY\nsetosa\tsetosa\t0.9901343905576048\nsetosa\tsetosa\t0.929515574357658\nsetosa\tsetosa\t0.9768849788903005\nsetosa\tsetosa\t0.9537699577806007\nsetosa\tsetosa\t0.9288797604903923\nsetosa\tsetosa\t0.971193515419979\nsetosa\tsetosa\t0.9672745643609024\nsetosa\tsetosa\t0.934908418392961\nsetosa\tsetosa\t0.9860059524324714\nsetosa\tsetosa\t0.9672745643609025\nsetosa\tversicolor\t0.7195105245248212\nsetosa\tversicolor\t0.7144151490663857\nsetosa\tversicolor\t0.7322404630087489\nsetosa\tversicolor\t0.6838602129585816\nsetosa\tversicolor\t0.7395288035652832\nsetosa\tversicolor\t0.7228123473134247\nsetosa\tversicolor\t0.77530441046781\nsetosa\tversicolor\t0.7375016656329502\nsetosa\tversicolor\t0.7307572255018732\nsetosa\tversicolor\t0.7601793316654778\nsetosa\tversicolor\t0.7628166063189693\nsetosa\tvirginica\t0.6127070341693636\nsetosa\tvirginica\t0.6298957464050077\nsetosa\tvirginica\t0.5359246172328769\nsetosa\tvirginica\t0.6268784272440107\nsetosa\tvirginica\t0.6891755049408106\nsetosa\tvirginica\t0.628043138178681\nsetosa\tvirginica\t0.634447023784445\nsetosa\tvirginica\t0.5946976225332523\nsetosa\tvirginica\t0.7136085414545078\nsetosa\tvirginica\t0.7059417012750165\nsetosa\tvirginica\t0.6073208299662805\nsetosa\tvirginica\t0.527662856324685\nsetosa\tvirginica\t0.7140846622615968\nsetosa\tvirginica\t0.6482002080086136\nsetosa\tvirginica\t0.6833192544018778\nsetosa\tvirginica\t0.6637712856218627\nsetosa\tvirginica\t0.6860309899100281\nsetosa\tsetosa\t0.9358264864420958\nsetosa\tsetosa\t0.9836372821804513\nsetosa\tsetosa\t0.9611830549757171\nsetosa\tsetosa\t0.9223661099514342\nsetosa\tsetosa\t0.9672745643609024\nsetosa\tsetosa\t0.9611830549757171\nsetosa\tsetosa\t0.9401709929520021\nsetosa\tsetosa\t0.9836372821804511\nsetosa\tsetosa\t0.971193515419979\nsetosa\tversicolor\t0.7259907458223617\nsetosa\tversicolor\t0.7204742086636562\nsetosa\tversicolor\t0.741594433263048\nsetosa\tversicolor\t0.6903051190131508\nsetosa\tversicolor\t0.7478086973938867\nsetosa\tversicolor\t0.732469083582495\nsetosa\tversicolor\t0.7846620277192069\nsetosa\tversicolor\t0.7469998658124829\nsetosa\tversicolor\t0.7388076274564386\nsetosa\tversicolor\t0.7694003431717377\nsetosa\tversicolor\t0.7717732565545045\nsetosa\tvirginica\t0.6213988258335706\nsetosa\tvirginica\t0.637994533108029\nsetosa\tvirginica\t0.5420250377902858\nsetosa\tvirginica\t0.634447023784445\nsetosa\tvirginica\t0.6985268388068894\nsetosa\tvirginica\t0.6349050688477991\nsetosa\tvirginica\t0.6419037709168984\nsetosa\tvirginica\t0.6010812819262368\nsetosa\tvirginica\t0.7215583260332424\nsetosa\tvirginica\t0.7143147091527151\nsetosa\tvirginica\t0.6134092887478193\nsetosa\tvirginica\t0.5326944151147545\nsetosa\tvirginica\t0.7226575982759195\nsetosa\tvirginica\t0.6546169129209298\nsetosa\tvirginica\t0.6925337908956499\nsetosa\tvirginica\t0.6722936014510363\nsetosa\tvirginica\t0.6950499224101034\nsetosa\tsetosa\t0.9521107014612561\nsetosa\tsetosa\t0.9737831357188022\nsetosa\tsetosa\t0.8585058202337239\nsetosa\tsetosa\t0.9031786133920467\nsetosa\tsetosa\t0.8972251677012768\nsetosa\tsetosa\t0.9901915723990057\nsetosa\tsetosa\t0.9195104240346337\nsetosa\tsetosa\t0.9610724950996288\nsetosa\tversicolor\t0.7045999255203874\nsetosa\tversicolor\t0.6960141547504884\nsetosa\tversicolor\t0.7532760397654232\nsetosa\tversicolor\t0.6694454625977622\nsetosa\tversicolor\t0.7429589476514327\nsetosa\tversicolor\t0.7520345186365123\nsetosa\tversicolor\t0.794720811314474\nsetosa\tversicolor\t0.761739146656636\nsetosa\tversicolor\t0.731613684675603\nsetosa\tversicolor\t0.7774400533321789\nsetosa\tversicolor\t0.7751961565776003\nsetosa\tvirginica\t0.6247680497906658\nsetosa\tvirginica\t0.6336691649695381\nsetosa\tvirginica\t0.5205314990515972\nsetosa\tvirginica\t0.6245476222221473\nsetosa\tvirginica\t0.7113557289936998\nsetosa\tvirginica\t0.618466922269288\nsetosa\tvirginica\t0.6307833928320332\nsetosa\tvirginica\t0.5811567689841488\nsetosa\tvirginica\t0.7137708091232325\nsetosa\tvirginica\t0.7116106942227461\nsetosa\tvirginica\t0.5910368538647536\nsetosa\tvirginica\t0.5037075098546453\nsetosa\tvirginica\t0.7222753075830516\nsetosa\tvirginica\t0.6341856891971133\nsetosa\tvirginica\t0.7028383818489132\nsetosa\tvirginica\t0.6724729590370829\nsetosa\tvirginica\t0.7019115888775074\nsetosa\tsetosa\t0.9768849788903002\nsetosa\tsetosa\t0.9063242906211434\nsetosa\tsetosa\t0.9509118465413539\nsetosa\tsetosa\t0.9450938681928978\nsetosa\tsetosa\t0.9564965969662069\nsetosa\tsetosa\t0.9672745643609024\nsetosa\tsetosa\t0.9860059524324714\nsetosa\tversicolor\t0.7204742086636562\nsetosa\tversicolor\t0.7141263535435329\nsetosa\tversicolor\t0.7447021830005207\nsetosa\tversicolor\t0.6847881213734297\nsetosa\tversicolor\t0.7467295914842007\nsetosa\tversicolor\t0.73750166563295\nsetosa\tversicolor\t0.7876684649772377\nsetosa\tversicolor\t0.7509089228999535\nsetosa\tversicolor\t0.7370681764285258\nsetosa\tversicolor\t0.7717732565545046\nsetosa\tversicolor\t0.7729886166135186\nsetosa\tvirginica\t0.6219098047362288\nsetosa\tvirginica\t0.6365846319905831\nsetosa\tvirginica\t0.5359954719184903\nsetosa\tvirginica\t0.6315943754452643\nsetosa\tvirginica\t0.7017066790074629\nsetosa\tvirginica\t0.6303674471977562\nsetosa\tvirginica\t0.6387578783089636\nsetosa\tvirginica\t0.5955636219082519\nsetosa\tvirginica\t0.7195745011444197\nsetosa\tvirginica\t0.7136085414545075\nsetosa\tvirginica\t0.607287014278362\nsetosa\tvirginica\t0.5247331825444475\nsetosa\tvirginica\t0.7225895671332754\nsetosa\tvirginica\t0.6490995890160414\nsetosa\tvirginica\t0.6950499224101033\nsetosa\tvirginica\t0.6721513066120526\nsetosa\tvirginica\t0.6967029393943086\nsetosa\tsetosa\t0.8835491649271512\nsetosa\tsetosa\t0.9288797604903923\nsetosa\tsetosa\t0.9223661099514342\nsetosa\tsetosa\t0.9751648121259539\nsetosa\tsetosa\t0.9450938681928974\nsetosa\tsetosa\t0.9802687811152093\nsetosa\tversicolor\t0.7195296832214126\nsetosa\tversicolor\t0.7119767634449206\nsetosa\tversicolor\t0.7556081231271513\nsetosa\tversicolor\t0.6840232598658664\nsetosa\tversicolor\t0.7519774931801742\nsetosa\tversicolor\t0.7509089228999535\nsetosa\tversicolor\t0.798134049486224\nsetosa\tversicolor\t0.7628166063189693\nsetosa\tversicolor\t0.7414594335977502\nsetosa\tversicolor\t0.7815290676340242\nsetosa\tversicolor\t0.7811827645320393\nsetosa\tvirginica\t0.6298957464050077\nsetosa\tvirginica\t0.6419037709168985\nsetosa\tvirginica\t0.5349160810438174\nsetosa\tvirginica\t0.6349050688477991\nsetosa\tvirginica\t0.7129206514384616\nsetosa\tvirginica\t0.6312546277876196\nsetosa\tvirginica\t0.6416267714387183\nsetosa\tvirginica\t0.5951096950166025\nsetosa\tvirginica\t0.7237293687508725\nsetosa\tvirginica\t0.7195745011444195\nsetosa\tvirginica\t0.6058951203902192\nsetosa\tvirginica\t0.5207053863696072\nsetosa\tvirginica\t0.7293819581507073\nsetosa\tvirginica\t0.6484780069269589\nsetosa\tvirginica\t0.7054308002974701\nsetosa\tvirginica\t0.679081955100999\nsetosa\tvirginica\t0.7059417012750167\nsetosa\tsetosa\t0.9537699577806006\nsetosa\tsetosa\t0.9611830549757171\nsetosa\tsetosa\t0.863818178651925\nsetosa\tsetosa\t0.9382393304003435\nsetosa\tsetosa\t0.8963113796488393\nsetosa\tversicolor\t0.7225895671332752\nsetosa\tversicolor\t0.7215583260332423\nsetosa\tversicolor\t0.700156602331854\nsetosa\tversicolor\t0.688406212124026\nsetosa\tversicolor\t0.722851009602718\nsetosa\tversicolor\t0.6843004978433498\nsetosa\tversicolor\t0.7418292536121499\nsetosa\tversicolor\t0.7024167166208548\nsetosa\tversicolor\t0.7171246762269271\nsetosa\tversicolor\t0.7296113169226035\nsetosa\tversicolor\t0.7362939031390524\nsetosa\tvirginica\t0.5936716087836178\nsetosa\tvirginica\t0.6182689599494733\nsetosa\tvirginica\t0.5465050981369312\nsetosa\tvirginica\t0.6213988258335706\nsetosa\tvirginica\t0.6581379995606758\nsetosa\tvirginica\t0.6298957464050077\nsetosa\tvirginica\t0.6300317347692546\nsetosa\tvirginica\t0.601783169522111\nsetosa\tvirginica\t0.7017066790074629\nsetosa\tvirginica\t0.6891755049408104\nsetosa\tvirginica\t0.6169068610102361\nsetosa\tvirginica\t0.5475429224658782\nsetosa\tvirginica\t0.6945025277360516\nsetosa\tvirginica\t0.6539168616953026\nsetosa\tvirginica\t0.654884202444555\nsetosa\tvirginica\t0.646109905207406\nsetosa\tvirginica\t0.6607602401039968\nsetosa\tsetosa\t0.9901343905576048\nsetosa\tsetosa\t0.9074804382378517\nsetosa\tsetosa\t0.9836372821804513\nsetosa\tsetosa\t0.9391378185275493\nsetosa\tversicolor\t0.7343304509598597\nsetosa\tversicolor\t0.7306248864563647\nsetosa\tversicolor\t0.732469083582495\nsetosa\tversicolor\t0.698964898590803\nsetosa\tversicolor\t0.746788643861224\nsetosa\tversicolor\t0.7197964064590316\nsetosa\tversicolor\t0.7751702942349055\nsetosa\tversicolor\t0.7362939031390524\nsetosa\tversicolor\t0.7392232565234864\nsetosa\tversicolor\t0.7613367570254728\nsetosa\tversicolor\t0.7659050745877365\nsetosa\tvirginica\t0.6182689599494733\nsetosa\tvirginica\t0.638601541266658\nsetosa\tvirginica\t0.5525301964608882\nsetosa\tvirginica\t0.637994533108029\nsetosa\tvirginica\t0.6896665965957882\nsetosa\tvirginica\t0.6419037709168984\nsetosa\tvirginica\t0.6460036467849122\nsetosa\tvirginica\t0.6102856247871346\nsetosa\tvirginica\t0.7226575982759194\nsetosa\tvirginica\t0.7129206514384614\nsetosa\tvirginica\t0.6238130956861374\nsetosa\tvirginica\t0.547260710062824\nsetosa\tvirginica\t0.7199111604110404\nsetosa\tvirginica\t0.6635291024917067\nsetosa\tvirginica\t0.68500199875954\nsetosa\tvirginica\t0.6701343445520873\nsetosa\tvirginica\t0.6891755049408104\nsetosa\tsetosa\t0.9022465138524667\nsetosa\tsetosa\t0.9768849788903005\nsetosa\tsetosa\t0.934549128721805\nsetosa\tversicolor\t0.7270310372988782\nsetosa\tversicolor\t0.7237293687508723\nsetosa\tversicolor\t0.7228123473134247\nsetosa\tversicolor\t0.6917889541967065\nsetosa\tversicolor\t0.7378945769863159\nsetosa\tversicolor\t0.7099780556068986\nsetosa\tversicolor\t0.7654695476548654\nsetosa\tversicolor\t0.7265485710372153\nsetosa\tversicolor\t0.73053164023153\nsetosa\tversicolor\t0.7517332772766305\nsetosa\tversicolor\t0.7564688505515823\nsetosa\tvirginica\t0.6092135492710526\nsetosa\tvirginica\t0.6300317347692547\nsetosa\tvirginica\t0.5459156567416428\nsetosa\tvirginica\t0.6298957464050077\nsetosa\tvirginica\t0.6800451720772858\nsetosa\tvirginica\t0.634447023784445\nsetosa\tvirginica\t0.637994533108029\nsetosa\tvirginica\t0.6033267860924852\nsetosa\tvirginica\t0.7140846622615966\nsetosa\tvirginica\t0.7040148300224642\nsetosa\tvirginica\t0.6171167144380281\nsetosa\tvirginica\t0.5416996583933069\nsetosa\tvirginica\t0.7108339552162921\nsetosa\tvirginica\t0.6564526924737581\nsetosa\tvirginica\t0.6754796824378368\nsetosa\tvirginica\t0.6611665804556707\nsetosa\tvirginica\t0.6797933057583387\nsetosa\tsetosa\t0.9238294600120065\nsetosa\tsetosa\t0.967442478384357\nsetosa\tversicolor\t0.6989703734119916\nsetosa\tversicolor\t0.6907636255641807\nsetosa\tversicolor\t0.7445025393432967\nsetosa\tversicolor\t0.6636701106705138\nsetosa\tversicolor\t0.7355670618764933\nsetosa\tversicolor\t0.7427668341095446\nsetosa\tversicolor\t0.7861738473559798\nsetosa\tversicolor\t0.7527754977776361\nsetosa\tversicolor\t0.7244271239570537\nsetosa\tversicolor\t0.7689979692795962\nsetosa\tversicolor\t0.7671311840656956\nsetosa\tvirginica\t0.6164296968831195\nsetosa\tvirginica\t0.6260149216568054\nsetosa\tvirginica\t0.5146351007190895\nsetosa\tvirginica\t0.6174359344236791\nsetosa\tvirginica\t0.7024333408562314\nsetosa\tvirginica\t0.6120694993276603\nsetosa\tvirginica\t0.6238095312267242\nsetosa\tvirginica\t0.5751284960461227\nsetosa\tvirginica\t0.706600484737697\nsetosa\tvirginica\t0.703934434700885\nsetosa\tvirginica\t0.5853288331785594\nsetosa\tvirginica\t0.4988046448121186\nsetosa\tvirginica\t0.7144077864116676\nsetosa\tvirginica\t0.6282938493576008\nsetosa\tvirginica\t0.6940752570706307\nsetosa\tvirginica\t0.6644637852151319\nsetosa\tvirginica\t0.6933997211991552\nsetosa\tsetosa\t0.9552867937933313\nsetosa\tversicolor\t0.7306248864563647\nsetosa\tversicolor\t0.7259907458223617\nsetosa\tversicolor\t0.73750166563295\nsetosa\tversicolor\t0.6950429747316682\nsetosa\tversicolor\t0.747828464216397\nsetosa\tversicolor\t0.726548571037215\nsetosa\tversicolor\t0.7804739630117439\nsetosa\tversicolor\t0.7421100076082314\nsetosa\tversicolor\t0.7395288035652832\nsetosa\tversicolor\t0.7659050745877366\nsetosa\tversicolor\t0.7694003431717373\nsetosa\tvirginica\t0.6201829657918358\nsetosa\tvirginica\t0.6386682085910342\nsetosa\tvirginica\t0.5475429224658785\nsetosa\tvirginica\t0.6365846319905831\nsetosa\tvirginica\t0.6945025277360513\nsetosa\tvirginica\t0.6387578783089636\nsetosa\tvirginica\t0.6443239885062291\nsetosa\tvirginica\t0.6059962177793947\nsetosa\tvirginica\t0.7225895671332753\nsetosa\tvirginica\t0.7140846622615964\nsetosa\tvirginica\t0.6189268522089035\nsetosa\tvirginica\t0.540210973788589\nsetosa\tvirginica\t0.7217617129262339\nsetosa\tvirginica\t0.6594367425562319\nsetosa\tvirginica\t0.6891755049408104\nsetosa\tvirginica\t0.6716196129295104\nsetosa\tvirginica\t0.6925337908956499\nsetosa\tversicolor\t0.7084892019459779\nsetosa\tversicolor\t0.7017821924326191\nsetosa\tversicolor\t0.7378945769863159\nsetosa\tversicolor\t0.6728355315417575\nsetosa\tversicolor\t0.7370681764285258\nsetosa\tversicolor\t0.7322404630087489\nsetosa\tversicolor\t0.7806518951329624\nsetosa\tversicolor\t0.7447021830005207\nsetosa\tversicolor\t0.7270310372988784\nsetosa\tversicolor\t0.7643446402379761\nsetosa\tversicolor\t0.7647418184207583\nsetosa\tvirginica\t0.6134517611397117\nsetosa\tvirginica\t0.6268784272440107\nsetosa\tvirginica\t0.5238894673880157\nsetosa\tvirginica\t0.6210485969702222\nsetosa\tvirginica\t0.6950499224101034\nsetosa\tvirginica\t0.6189268522089033\nsetosa\tvirginica\t0.6280431381786808\nsetosa\tvirginica\t0.5836680362928923\nsetosa\tvirginica\t0.7094239118972434\nsetosa\tvirginica\t0.704237842976878\nsetosa\tvirginica\t0.5951096950166027\nsetosa\tvirginica\t0.5118325494777062\nsetosa\tvirginica\t0.7136085414545079\nsetosa\tvirginica\t0.6371731224710983\nsetosa\tvirginica\t0.687907287765229\nsetosa\tvirginica\t0.6632013675500092\nsetosa\tvirginica\t0.6889330299885271\nversicolor\tversicolor\t0.9836372821804512\nversicolor\tversicolor\t0.857658745304188\nversicolor\tversicolor\t0.9643018936332983\nversicolor\tversicolor\t0.9228986606680837\nversicolor\tversicolor\t0.8249797059410379\nversicolor\tversicolor\t0.8581422634293595\nversicolor\tversicolor\t0.8457973213361673\nversicolor\tversicolor\t0.9345491287218052\nversicolor\tversicolor\t0.8690357671724774\nversicolor\tversicolor\t0.8847740616799162\nversicolor\tvirginica\t0.8422461577546221\nversicolor\tvirginica\t0.8816126866912564\nversicolor\tvirginica\t0.8153471612953149\nversicolor\tvirginica\t0.8950006662531802\nversicolor\tvirginica\t0.8447322199028685\nversicolor\tvirginica\t0.9071861086777475\nversicolor\tvirginica\t0.9044765202766962\nversicolor\tvirginica\t0.8750566277165437\nversicolor\tvirginica\t0.9382393304003436\nversicolor\tvirginica\t0.9152025130114022\nversicolor\tvirginica\t0.8863383568226034\nversicolor\tvirginica\t0.79661753120616\nversicolor\tvirginica\t0.9063242906211435\nversicolor\tvirginica\t0.9286037872665964\nversicolor\tvirginica\t0.8534541572849749\nversicolor\tvirginica\t0.887303803117376\nversicolor\tvirginica\t0.8690461684968342\nversicolor\tversicolor\t0.8413216160720383\nversicolor\tversicolor\t0.9661166580455672\nversicolor\tversicolor\t0.9066163558815064\nversicolor\tversicolor\t0.8086342397191548\nversicolor\tversicolor\t0.8427399791825902\nversicolor\tversicolor\t0.8295227155988006\nversicolor\tversicolor\t0.9181864109022564\nversicolor\tversicolor\t0.8531609017756384\nversicolor\tversicolor\t0.8690357671724773\nversicolor\tvirginica\t0.8309016166533585\nversicolor\tvirginica\t0.872273262437411\nversicolor\tvirginica\t0.8215094681664913\nversicolor\tvirginica\t0.8893091264582885\nversicolor\tvirginica\t0.8287207534785156\nversicolor\tvirginica\t0.9072539043087448\nversicolor\tvirginica\t0.8990886073834587\nversicolor\tvirginica\t0.8795859594363211\nversicolor\tvirginica\t0.9223661099514343\nversicolor\tvirginica\t0.8994616149924297\nversicolor\tvirginica\t0.8929056808998949\nversicolor\tvirginica\t0.8071743645545153\nversicolor\tvirginica\t0.8901877363857955\nversicolor\tvirginica\t0.9323454895376768\nversicolor\tvirginica\t0.8377103234409576\nversicolor\tvirginica\t0.8737918946056917\nversicolor\tvirginica\t0.853454157284975\nversicolor\tversicolor\t0.8402449656949386\nversicolor\tversicolor\t0.9345491287218051\nversicolor\tversicolor\t0.9672745643609024\nversicolor\tversicolor\t0.9569324055438412\nversicolor\tversicolor\t0.9860059524324712\nversicolor\tversicolor\t0.9228986606680836\nversicolor\tversicolor\t0.970403171672814\nversicolor\tversicolor\t0.9593623692639385\nversicolor\tvirginica\t0.868665405926465\nversicolor\tvirginica\t0.8645928741918691\nversicolor\tvirginica\t0.7190388303883071\nversicolor\tvirginica\t0.8430142991837434\nversicolor\tvirginica\t0.9569324055438414\nversicolor\tvirginica\t0.8187194349799699\nversicolor\tvirginica\t0.844845742553702\nversicolor\tvirginica\t0.7767873861911752\nversicolor\tvirginica\t0.9135805462599372\nversicolor\tvirginica\t0.9302430002080333\nversicolor\tvirginica\t0.7768657123633214\nversicolor\tvirginica\t0.6793537342902706\nversicolor\tvirginica\t0.9440628585229267\nversicolor\tvirginica\t0.8174321529414851\nversicolor\tvirginica\t0.9495617394787773\nversicolor\tvirginica\t0.9084378628660885\nversicolor\tvirginica\t0.9464528404499473\nversicolor\tversicolor\t0.9018714881302554\nversicolor\tversicolor\t0.8084777650737703\nversicolor\tversicolor\t0.8327303855690306\nversicolor\tversicolor\t0.8271610925198742\nversicolor\tversicolor\t0.9155356807185353\nversicolor\tversicolor\t0.8459455227237029\nversicolor\tversicolor\t0.8604860004160664\nversicolor\tvirginica\t0.8558255980083322\nversicolor\tvirginica\t0.8999607237867151\nversicolor\tvirginica\t0.8508910962163098\nversicolor\tvirginica\t0.9210751244608374\nversicolor\tvirginica\t0.8363728218045123\nversicolor\tvirginica\t0.941086160059494\nversicolor\tvirginica\t0.9309407339032326\nversicolor\tvirginica\t0.9107547340832454\nversicolor\tvirginica\t0.926580450887819\nversicolor\tvirginica\t0.9066163558815062\nversicolor\tvirginica\t0.921557774965682\nversicolor\tvirginica\t0.829121306552622\nversicolor\tvirginica\t0.8944575282112031\nversicolor\tvirginica\t0.9643018936332981\nversicolor\tvirginica\t0.8473328120040373\nversicolor\tvirginica\t0.8921322305247363\nversicolor\tvirginica\t0.8636677595420372\nversicolor\tversicolor\t0.9018236930827075\nversicolor\tversicolor\t0.9300297621623569\nversicolor\tversicolor\t0.9228986606680836\nversicolor\tversicolor\t0.9860059524324714\nversicolor\tversicolor\t0.9440628585229267\nversicolor\tversicolor\t0.9580178572974142\nversicolor\tvirginica\t0.8707972166315239\nversicolor\tvirginica\t0.8898093766968946\nversicolor\tvirginica\t0.7663322767683983\nversicolor\tvirginica\t0.8809723668470038\nversicolor\tvirginica\t0.9140847260170017\nversicolor\tvirginica\t0.867996393121323\nversicolor\tvirginica\t0.8863383568226031\nversicolor\tvirginica\t0.8268200932173946\nversicolor\tvirginica\t0.9694792876220294\nversicolor\tvirginica\t0.9661166580455668\nversicolor\tvirginica\t0.8308774523173391\nversicolor\tvirginica\t0.734113096216956\nversicolor\tvirginica\t0.9704031716728142\nversicolor\tvirginica\t0.8740535718922422\nversicolor\tvirginica\t0.9187247385278764\nversicolor\tvirginica\t0.9231334477239126\nversicolor\tvirginica\t0.9308115614911228\nversicolor\tversicolor\t0.9404418615121475\nversicolor\tversicolor\t0.9768849788903003\nversicolor\tversicolor\t0.8903101166022681\nversicolor\tversicolor\t0.9455726274239105\nversicolor\tversicolor\t0.930654936670901\nversicolor\tvirginica\t0.8559883817224604\nversicolor\tvirginica\t0.8430142991837435\nversicolor\tvirginica\t0.6928822241915299\nversicolor\tvirginica\t0.8180773816221278\nversicolor\tvirginica\t0.9519961272408202\nversicolor\tvirginica\t0.7907290006240997\nversicolor\tvirginica\t0.8187194349799698\nversicolor\tvirginica\t0.749095578353546\nversicolor\tvirginica\t0.881891624139636\nversicolor\tvirginica\t0.9003110812965137\nversicolor\tvirginica\t0.7478194896018702\nversicolor\tvirginica\t0.6505919560427509\nversicolor\tvirginica\t0.9135805462599371\nversicolor\tvirginica\t0.7872133951440392\nversicolor\tvirginica\t0.9389585752440589\nversicolor\tvirginica\t0.8867124613265265\nversicolor\tvirginica\t0.9284437993918346\nversicolor\tversicolor\t0.9605375622304184\nversicolor\tversicolor\t0.9160357145948282\nversicolor\tversicolor\t0.9821509468166489\nversicolor\tversicolor\t0.9720119048649427\nversicolor\tvirginica\t0.8297363826712159\nversicolor\tvirginica\t0.8331273560013681\nversicolor\tvirginica\t0.6975675200997646\nversicolor\tvirginica\t0.8168757257321766\nversicolor\tvirginica\t0.9138648110876826\nversicolor\tvirginica\t0.7991837513812968\nversicolor\tvirginica\t0.8206713461569821\nversicolor\tvirginica\t0.7575778586572421\nversicolor\tvirginica\t0.9006463849358767\nversicolor\tvirginica\t0.9084378628660881\nversicolor\tvirginica\t0.7609907955555886\nversicolor\tvirginica\t0.664142858379313\nversicolor\tvirginica\t0.921557774965682\nversicolor\tvirginica\t0.8040833340545992\nversicolor\tvirginica\t0.9072539043087443\nversicolor\tvirginica\t0.8750566277165437\nversicolor\tvirginica\t0.9071861086777472\nversicolor\tversicolor\t0.9105735875866625\nversicolor\tversicolor\t0.9684479736514227\nversicolor\tversicolor\t0.9537699577806007\nversicolor\tvirginica\t0.8572075745331929\nversicolor\tvirginica\t0.8511456281161294\nversicolor\tvirginica\t0.7050529336095495\nversicolor\tvirginica\t0.829121306552622\nversicolor\tvirginica\t0.9495617394787774\nversicolor\tvirginica\t0.8047444774426795\nversicolor\tvirginica\t0.8308774523173391\nversicolor\tvirginica\t0.762806098302566\nversicolor\tvirginica\t0.9003110812965137\nversicolor\tvirginica\t0.9163651927845152\nversicolor\tvirginica\t0.7629870547984509\nversicolor\tvirginica\t0.6654607711380613\nversicolor\tvirginica\t0.9302430002080333\nversicolor\tvirginica\t0.803742976260511\nversicolor\tvirginica\t0.9397929797181607\nversicolor\tvirginica\t0.8950917830703029\nversicolor\tvirginica\t0.9343158533983699\nversicolor\tversicolor\t0.9302430002080333\nversicolor\tversicolor\t0.9440238097298855\nversicolor\tvirginica\t0.875866638638315\nversicolor\tvirginica\t0.8991234789575545\nversicolor\tvirginica\t0.7799939885355386\nversicolor\tvirginica\t0.8929056808998945\nversicolor\tvirginica\t0.9075399155612012\nversicolor\tvirginica\t0.8815959893548698\nversicolor\tvirginica\t0.8987774237541505\nversicolor\tvirginica\t0.8405922291192485\nversicolor\tvirginica\t0.982150946816649\nversicolor\tvirginica\t0.9704031716728138\nversicolor\tvirginica\t0.8448457425537022\nversicolor\tvirginica\t0.7481071437844847\nversicolor\tvirginica\t0.9684479736514229\nversicolor\tvirginica\t0.888047619459771\nversicolor\tvirginica\t0.9140847260170014\nversicolor\tvirginica\t0.9282853489236959\nversicolor\tvirginica\t0.9279127990041659\nversicolor\tversicolor\t0.9836372821804514\nversicolor\tvirginica\t0.8472566644164387\nversicolor\tvirginica\t0.8508910962163097\nversicolor\tvirginica\t0.7137751975673388\nversicolor\tvirginica\t0.8340859462613632\nversicolor\tvirginica\t0.9282853489236959\nversicolor\tvirginica\t0.8152130626303925\nversicolor\tvirginica\t0.8376059573971113\nversicolor\tvirginica\t0.7734249226530528\nversicolor\tvirginica\t0.9160357145948284\nversicolor\tvirginica\t0.9255728140580646\nversicolor\tvirginica\t0.7760952389195421\nversicolor\tvirginica\t0.6788950254753234\nversicolor\tvirginica\t0.938958575244059\nversicolor\tvirginica\t0.8187194349799699\nversicolor\tvirginica\t0.9231334477239124\nversicolor\tvirginica\t0.8929056808998947\nversicolor\tvirginica\t0.9243426092181659\nversicolor\tvirginica\t0.848685218436332\nversicolor\tvirginica\t0.857207574533193\nversicolor\tvirginica\t0.7251464304628257\nversicolor\tvirginica\t0.843115549931364\nversicolor\tvirginica\t0.9210751244608376\nversicolor\tvirginica\t0.8268200932173947\nversicolor\tvirginica\t0.8473964381101472\nversicolor\tvirginica\t0.7853313981755039\nversicolor\tvirginica\t0.9284437993918347\nversicolor\tvirginica\t0.9343158533983698\nversicolor\tvirginica\t0.7889521046410879\nversicolor\tvirginica\t0.6921309535143703\nversicolor\tvirginica\t0.9464528404499476\nversicolor\tvirginica\t0.8320714291896565\nversicolor\tvirginica\t0.9185851978655344\nversicolor\tvirginica\t0.8972629847322356\nversicolor\tvirginica\t0.9231334477239126\nvirginica\tvirginica\t0.9537699577806007\nvirginica\tvirginica\t0.8114493733788214\nvirginica\tvirginica\t0.9223661099514342\nvirginica\tvirginica\t0.9039922544816402\nvirginica\tvirginica\t0.8835491649271513\nvirginica\tvirginica\t0.9152025130114021\nvirginica\tvirginica\t0.8527355396240612\nvirginica\tvirginica\t0.8893091264582883\nvirginica\tvirginica\t0.9044765202766962\nvirginica\tvirginica\t0.8414995905661047\nvirginica\tvirginica\t0.7565512741101975\nvirginica\tvirginica\t0.8983499741367009\nvirginica\tvirginica\t0.861309873341802\nvirginica\tvirginica\t0.9165636780006841\nvirginica\tvirginica\t0.9475003331265899\nvirginica\tvirginica\t0.922057124957427\nvirginica\tvirginica\t0.8459455227237029\nvirginica\tvirginica\t0.9672745643609024\nvirginica\tvirginica\t0.8880476194597711\nvirginica\tvirginica\t0.9288797604903922\nvirginica\tvirginica\t0.961183054975717\nvirginica\tvirginica\t0.8944575282112031\nvirginica\tvirginica\t0.9159428214054656\nvirginica\tvirginica\t0.9220571249574271\nvirginica\tvirginica\t0.8854609752631586\nvirginica\tvirginica\t0.7956354562835347\nvirginica\tvirginica\t0.9107547340832454\nvirginica\tvirginica\t0.9075399155612013\nvirginica\tvirginica\t0.9020416670272996\nvirginica\tvirginica\t0.9559987977071077\nvirginica\tvirginica\t0.9145606532763111\nvirginica\tvirginica\t0.8745766877338075\nvirginica\tvirginica\t0.7351933004151612\nvirginica\tvirginica\t0.89830876560308\nvirginica\tvirginica\t0.8740535718922424\nvirginica\tvirginica\t0.9389585752440591\nvirginica\tvirginica\t0.79661753120616\nvirginica\tvirginica\t0.7873627945944426\nvirginica\tvirginica\t0.9286037872665964\nvirginica\tvirginica\t0.9382393304003438\nvirginica\tvirginica\t0.7734249226530528\nvirginica\tvirginica\t0.8863383568226034\nvirginica\tvirginica\t0.7490955783535459\nvirginica\tvirginica\t0.8049083326165811\nvirginica\tvirginica\t0.7631018966979424\nvirginica\tvirginica\t0.8604860004160665\nvirginica\tvirginica\t0.9611830549757171\nvirginica\tvirginica\t0.9901343905576048\nvirginica\tvirginica\t0.9265804508878193\nvirginica\tvirginica\t0.9107547340832455\nvirginica\tvirginica\t0.9084378628660884\nvirginica\tvirginica\t0.9181864109022562\nvirginica\tvirginica\t0.8271610925198745\nvirginica\tvirginica\t0.8950917830703026\nvirginica\tvirginica\t0.9368959473028459\nvirginica\tvirginica\t0.8743251327198516\nvirginica\tvirginica\t0.9302430002080332\nvirginica\tvirginica\t0.8885223560623275\nvirginica\tvirginica\t0.8293900466468787\nvirginica\tvirginica\t0.8596555790109359\nvirginica\tvirginica\t0.7888392017963384\nvirginica\tvirginica\t0.9063242906211434\nvirginica\tvirginica\t0.9288797604903924\nvirginica\tvirginica\t0.7851464307636805\nvirginica\tvirginica\t0.6892948805795904\nvirginica\tvirginica\t0.9382393304003432\nvirginica\tvirginica\t0.8211471751733255\nvirginica\tvirginica\t0.9860059524324715\nvirginica\tvirginica\t0.9302430002080333\nvirginica\tvirginica\t0.9711935154199791\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t0.9580178572974141\nvirginica\tvirginica\t0.8983087656030799\nvirginica\tvirginica\t0.8880476194597711\nvirginica\tvirginica\t0.9552867937933314\nvirginica\tvirginica\t0.859655579010936\nvirginica\tvirginica\t0.8740535718922422\nvirginica\tvirginica\t0.9704031716728142\nvirginica\tvirginica\t0.84273997918259\nvirginica\tvirginica\t0.8978177281417673\nvirginica\tvirginica\t0.8581422634293594\nvirginica\tvirginica\t0.9290711317146798\nvirginica\tvirginica\t0.916563678000684\nvirginica\tvirginica\t0.9119975954142154\nvirginica\tvirginica\t0.9228986606680836\nvirginica\tvirginica\t0.8293900466468789\nvirginica\tvirginica\t0.8983087656030798\nvirginica\tvirginica\t0.9455726274239107\nvirginica\tvirginica\t0.8733035210778028\nvirginica\tvirginica\t0.9290711317146797\nvirginica\tvirginica\t0.8881257170458532\nvirginica\tvirginica\t0.8568875987836692\nvirginica\tvirginica\t0.8460654767571851\nvirginica\tvirginica\t0.980268781115209\nvirginica\tvirginica\t0.9003110812965138\nvirginica\tvirginica\t0.8320714291896564\nvirginica\tvirginica\t0.9464528404499473\nvirginica\tvirginica\t0.8024225709793451\nvirginica\tvirginica\t0.8581422634293594\nvirginica\tvirginica\t0.817432152941485\nvirginica\tvirginica\t0.9768849788903002\nvirginica\tvirginica\t0.8600595243247138\nvirginica\tvirginica\t0.762806098302566\nvirginica\tvirginica\t0.9672745643609025\nvirginica\tvirginica\t0.9024541663082905\nvirginica\tvirginica\t0.915202513011402\nvirginica\tvirginica\t0.9408063433456282\nvirginica\tvirginica\t0.9306549366709008\nvirginica\tvirginica\t0.8466187539867628\nvirginica\tvirginica\t0.749095578353546\nvirginica\tvirginica\t0.9860059524324712\nvirginica\tvirginica\t0.8868668670350979\nvirginica\tvirginica\t0.9382393304003432\nvirginica\tvirginica\t0.9569324055438416\nvirginica\tvirginica\t0.9537699577806006\nvirginica\tvirginica\t0.9024541663082906\nvirginica\tvirginica\t0.8327303855690307\nvirginica\tvirginica\t0.9559987977071078\nvirginica\tvirginica\t0.7983546079398532\nvirginica\tvirginica\t0.8531609017756383\nvirginica\tvirginica\t0.8139447081465295\nvirginica\tvirginica\t0.7351933004151612\nvirginica\tvirginica\t0.8600595243247138\nvirginica\tvirginica\t0.7028127911575033\nvirginica\tvirginica\t0.7584644113872434\nvirginica\tvirginica\t0.7179662828565849\nvirginica\tvirginica\t0.8733035210778031\nvirginica\tvirginica\t0.9455726274239102\nvirginica\tvirginica\t0.9495617394787772\nvirginica\tvirginica\t0.959362369263938\nvirginica\tvirginica\t0.8336041882724539\nvirginica\tvirginica\t0.8854609752631588\nvirginica\tvirginica\t0.8497576518169639\nvirginica\tvirginica\t0.9440628585229266\nvirginica\tvirginica\t0.9836372821804511\nvirginica\tvirginica\t0.9580178572974142\n","type":"TABLE"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":6,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":null,"message":["%md","","## Example using Digits dataset","","Use NMF to transform the predictors and then build two neural network models: one using the transformed data and one using the original data to compare confusion matrices. ","","This illustrates the process and you'll notice that the results of pipelining NMF as a preprocessing step before building a neural network does not provide better results. However, it did reduce the number of predictors fed to the neural network from 64 to 16, whcih may have other benefits, including performance, especially at larger scales. "],"enabled":true,"result":{"startTime":1737152713009,"interpreter":"md.low","endTime":1737152713457,"results":[{"message":"<h2 id=\"example-using-digits-dataset\">Example using Digits dataset<\/h2>\n<p>Use NMF to transform the predictors and then build two neural network models: one using the transformed data and one using the original data to compare confusion matrices.<\/p>\n<p>This illustrates the process and you'll notice that the results of pipelining NMF as a preprocessing step before building a neural network does not provide better results. However, it did reduce the number of predictors fed to the neural network from 64 to 16, whcih may have other benefits, including performance, especially at larger scales.<\/p>\n","type":"HTML"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":true,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":false,"hideVizConfig":true,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"Create table containing dataset digits from sklearn and generate a unique identifier column","message":["%python","","import oml","import pandas as pd","from sklearn import datasets ","import numpy as np","","digits = datasets.load_digits()","a = []","for i in range(0,digits.data.shape[1]):","    a.append(\"A\"+str(i))","","x = pd.DataFrame(digits.data.astype(np.float64), columns=a)","x.insert(0, 'ID', range(1, len(x) + 1))","y = pd.DataFrame(digits.target.astype(np.float64), columns=['Digit'])","","try: ","  oml.drop(table='DIGITS_NMF')","except: ","  print(\"Table DIGITS_NMF not found\")","  ","DIGITS = oml.create(pd.concat([x, y], axis=1), table = 'DIGITS_NMF')"],"enabled":true,"result":{"startTime":1737152713975,"interpreter":"python.low","endTime":1737152715051,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"table","title":"View a few rows of the data","message":["%python","","z.show(DIGITS.head())"],"enabled":true,"result":{"startTime":1737152715512,"interpreter":"python.low","endTime":1737152716123,"results":[{"message":"ID\tA0\tA1\tA2\tA3\tA4\tA5\tA6\tA7\tA8\tA9\tA10\tA11\tA12\tA13\tA14\tA15\tA16\tA17\tA18\tA19\tA20\tA21\tA22\tA23\tA24\tA25\tA26\tA27\tA28\tA29\tA30\tA31\tA32\tA33\tA34\tA35\tA36\tA37\tA38\tA39\tA40\tA41\tA42\tA43\tA44\tA45\tA46\tA47\tA48\tA49\tA50\tA51\tA52\tA53\tA54\tA55\tA56\tA57\tA58\tA59\tA60\tA61\tA62\tA63\tDigit\n1\t0\t0\t5\t13\t9\t1\t0\t0\t0\t0\t13\t15\t10\t15\t5\t0\t0\t3\t15\t2\t0\t11\t8\t0\t0\t4\t12\t0\t0\t8\t8\t0\t0\t5\t8\t0\t0\t9\t8\t0\t0\t4\t11\t0\t1\t12\t7\t0\t0\t2\t14\t5\t10\t12\t0\t0\t0\t0\t6\t13\t10\t0\t0\t0\t0\n2\t0\t0\t0\t12\t13\t5\t0\t0\t0\t0\t0\t11\t16\t9\t0\t0\t0\t0\t3\t15\t16\t6\t0\t0\t0\t7\t15\t16\t16\t2\t0\t0\t0\t0\t1\t16\t16\t3\t0\t0\t0\t0\t1\t16\t16\t6\t0\t0\t0\t0\t1\t16\t16\t6\t0\t0\t0\t0\t0\t11\t16\t10\t0\t0\t1\n3\t0\t0\t0\t4\t15\t12\t0\t0\t0\t0\t3\t16\t15\t14\t0\t0\t0\t0\t8\t13\t8\t16\t0\t0\t0\t0\t1\t6\t15\t11\t0\t0\t0\t1\t8\t13\t15\t1\t0\t0\t0\t9\t16\t16\t5\t0\t0\t0\t0\t3\t13\t16\t16\t11\t5\t0\t0\t0\t0\t3\t11\t16\t9\t0\t2\n4\t0\t0\t7\t15\t13\t1\t0\t0\t0\t8\t13\t6\t15\t4\t0\t0\t0\t2\t1\t13\t13\t0\t0\t0\t0\t0\t2\t15\t11\t1\t0\t0\t0\t0\t0\t1\t12\t12\t1\t0\t0\t0\t0\t0\t1\t10\t8\t0\t0\t0\t8\t4\t5\t14\t9\t0\t0\t0\t7\t13\t13\t9\t0\t0\t3\n5\t0\t0\t0\t1\t11\t0\t0\t0\t0\t0\t0\t7\t8\t0\t0\t0\t0\t0\t1\t13\t6\t2\t2\t0\t0\t0\t7\t15\t0\t9\t8\t0\t0\t5\t16\t10\t0\t16\t6\t0\t0\t4\t15\t16\t13\t16\t1\t0\t0\t0\t0\t3\t15\t10\t0\t0\t0\t0\t0\t2\t16\t4\t0\t0\t4\n","type":"TABLE"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"Build an NMF model to produce 16 features","message":["%python","","settings = {'FEAT_NUM_FEATURES':16}","nmf_mod_digits = oml.nmf(**settings)","nmf_mod_digits = nmf_mod_digits.fit(DIGITS.drop('Digit'), case_id = 'ID')"],"enabled":true,"result":{"startTime":1737152716600,"interpreter":"python.low","endTime":1737152718486,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"table","title":"Use the 'transform' function to produce the reduced feature space to use for modeling with the neural network algorithm","message":["%python","","DIGITS_TRANSFORMED = nmf_mod_digits.transform(DIGITS, supplemental_cols = DIGITS[:, ['ID','Digit']])","","z.show(DIGITS_TRANSFORMED.head(10))"],"enabled":true,"result":{"startTime":1737152718941,"interpreter":"python.low","endTime":1737152720140,"results":[{"message":"ID\tDigit\t'1'\t'2'\t'3'\t'4'\t'5'\t'6'\t'7'\t'8'\t'9'\t'10'\t'11'\t'12'\t'13'\t'14'\t'15'\t'16'\n1.0\t0.0\t0.0\t1.6071504178640184\t1.002599166254152\t1.6250586968971266\tnan\t0.6383334262939638\tnan\tnan\t0.4489665355514332\tnan\t3.8499678436437206\t1.9336331127355935\tnan\t2.2143628851526653\t1.5452149105539648\tnan\n2.0\t1.0\t1.5721304072406956\tnan\t0.47091145820610525\tnan\t0.3205784621943756\t1.320089706218631\t2.766609568935462\t3.0424340682087294\tnan\t2.2904106005997615\tnan\t1.0318897086132979\t2.1326345762323156\tnan\tnan\t0.9160036831823531\n3.0\t2.0\t0.5725843315651238\tnan\t1.225549348721752\t1.9011651955360889\tnan\tnan\t1.7694869170415073\t0.6167196913804647\t0.9914307121462165\t1.4902295100805643\t0.7296931460656757\tnan\t1.3912501600688703\t1.2414607870931396\tnan\tnan\n4.0\t3.0\t0.0\t0.0\tnan\tnan\t0.2521646586899621\tnan\t2.3232424893643118\t2.3789403834776115\tnan\t0.7315338821318872\t0.9405320997877227\t3.4764132163802617\tnan\t2.4939113192694657\tnan\t2.8254895691552497\n5.0\t4.0\t0.2723255065871001\t0.0\t4.371911041493471\t2.380459979440643\tnan\t1.0108417577550652\t1.160166864695847\t3.182334042986028\tnan\t3.8986837453813825\tnan\t1.3756796568582768\tnan\tnan\t0.5939766660870788\tnan\n6.0\t5.0\t1.494530101935104\tnan\tnan\t1.761737438558485\t1.7898300516326007\tnan\t2.1569907285842245\t0.9355171207172428\tnan\tnan\t3.648034167263212\t0.745274498114143\tnan\t2.198762923393277\t1.313588716556601\t0.20652339255341312\n7.0\t6.0\t0.8139881990267508\tnan\t1.4953534411192644\t0.3759157749071318\tnan\t0.4259014673596216\t0.8134758713575312\t2.2999734694390344\tnan\t2.3958535476013805\t2.332573371072175\t3.9273284181493535\tnan\tnan\tnan\t0.17825925453999997\n8.0\t7.0\tnan\t2.6086635289324285\t0.8437846760069846\tnan\t2.5602977787743653\t5.443138649445814\t1.2456694913756754\tnan\t1.1364935411888273\t0.2128601552403819\tnan\tnan\t4.992551751082081\tnan\t1.5849903888814028\t1.0247699788716127\n9.0\t8.0\tnan\tnan\tnan\t1.2188235207234637\t4.168366986465387\tnan\t0.5232456117170995\t0.8125652294908741\t1.6746763345681743\t0.8162256731700024\tnan\t0.5219773403153822\tnan\t1.655567094305008\t1.4564045216852468\t0.09236467826669013\n10.0\t9.0\t0.0\t0.0\t0.22816075076310138\t3.5243255692897306\t2.8187556831795044\tnan\t1.4821820076086663\tnan\t0.5849091822379927\tnan\t1.728146036507437\tnan\tnan\t2.3186566584133876\t2.8988704023113474\tnan\n","type":"TABLE"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"Split and prepare the transformed data for modeling","message":["%python","","DIGITS_TRANSFORMED_SPLIT = DIGITS_TRANSFORMED.split(ratio = (0.6,0.4))","DIGITS_TRANSFORMED_TRAIN = DIGITS_TRANSFORMED_SPLIT[0]","DIGITS_TRANSFORMED_TEST  = DIGITS_TRANSFORMED_SPLIT[1]","","DIGITS_TRANSFORMED_TRAIN_X = DIGITS_TRANSFORMED_TRAIN.drop('Digit')","DIGITS_TRANSFORMED_TRAIN_Y = DIGITS_TRANSFORMED_TRAIN['Digit']"],"enabled":true,"result":{"startTime":1737152720622,"interpreter":"python.low","endTime":1737152722009,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"Split and prepare the original data for modeling","message":["%python","","DIGITS_SPLIT = DIGITS.split(ratio = (0.6,0.4))","DIGITS_TRAIN = DIGITS_SPLIT[0]","DIGITS_TEST  = DIGITS_SPLIT[1]","","DIGITS_TRAIN_X = DIGITS_TRAIN.drop('Digit')","DIGITS_TRAIN_Y = DIGITS_TRAIN['Digit']"],"enabled":true,"result":{"startTime":1737152722517,"interpreter":"python.low","endTime":1737152723280,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":"[{\"raw\":{\"height\":300,\"lastColumns\":[],\"version\":1}}]","hideInIFrame":false,"selectedVisualization":"raw","title":"Build a 2-layer neural network using the transformed data","message":["%python","","try:","    oml.drop(model = 'NN_DIGITS_TRANSFORMED_MODEL') ","except:","    pass","","settings = {'NNET_HIDDEN_LAYERS':'2',","            'NNET_NODES_PER_LAYER':'8,4',","            'NNET_ACTIVATIONS':\"'NNET_ACTIVATIONS_BIPOLAR_SIG','NNET_ACTIVATIONS_TANH'\"}","","nn_mod_digits_transformed = oml.nn('classification', **settings)","nn_mod_digits_transformed.fit(DIGITS_TRANSFORMED_TRAIN_X, DIGITS_TRANSFORMED_TRAIN_Y, case_id = 'ID', model_name = 'NN_DIGITS_TRANSFORMED_MODEL')"],"enabled":true,"result":{"startTime":1737152723753,"interpreter":"python.low","endTime":1737152727007,"results":[{"message":"\nModel Name: NN_DIGITS_TRANSFORMED_MODEL\n\nModel Owner: OMLUSER\n\nAlgorithm Name: Neural Network\n\nMining Function: CLASSIFICATION\n\nTarget: Digit\n\nSettings: \n                    setting name                                      setting value\n0                      ALGO_NAME                                ALGO_NEURAL_NETWORK\n1          CLAS_WEIGHTS_BALANCED                                                OFF\n2       LBFGS_GRADIENT_TOLERANCE                                         .000000001\n3            LBFGS_HISTORY_DEPTH                                                 20\n4            LBFGS_SCALE_HESSIAN                         LBFGS_SCALE_HESSIAN_ENABLE\n5               NNET_ACTIVATIONS  'NNET_ACTIVATIONS_BIPOLAR_SIG','NNET_ACTIVATIO...\n6             NNET_HIDDEN_LAYERS                                                  2\n7                NNET_ITERATIONS                                                200\n8           NNET_NODES_PER_LAYER                                                8,4\n9                 NNET_TOLERANCE                                            .000001\n10                  ODMS_DETAILS                                        ODMS_ENABLE\n11  ODMS_MISSING_VALUE_TREATMENT                            ODMS_MISSING_VALUE_AUTO\n12              ODMS_RANDOM_SEED                                                  0\n13                 ODMS_SAMPLING                              ODMS_SAMPLING_DISABLE\n14                     PREP_AUTO                                                 ON\n\nComputed Settings: \n       setting name          setting value\n0  NNET_REGULARIZER  NNET_REGULARIZER_NONE\n\nGlobal Statistics: \n  attribute name attribute value\n0      CONVERGED              NO\n1     ITERATIONS             200\n2     LOSS_VALUE         1.07929\n3       NUM_ROWS            1075\n\nAttributes: \n'1'\n'10'\n'11'\n'12'\n'13'\n'14'\n'15'\n'16'\n'2'\n'3'\n'4'\n'5'\n'6'\n'7'\n'8'\n'9'\n\nPartition: NO\n\nTopology: \n\n   HIDDEN_LAYER_ID  NUM_NODE           ACTIVATION_FUNCTION\n0                0         8  NNET_ACTIVATIONS_BIPOLAR_SIG\n1                1         4         NNET_ACTIVATIONS_TANH\n\nWeights: \n\n     LAYER  IDX_FROM  IDX_TO  ... ATTRIBUTE_VALUE TARGET_VALUE    WEIGHT\n0        0       0.0       0  ...            None          NaN  0.212613\n1        0       0.0       1  ...            None          NaN  0.561249\n2        0       0.0       2  ...            None          NaN  0.265906\n3        0       0.0       3  ...            None          NaN  1.727418\n4        0       0.0       4  ...            None          NaN -2.343735\n..     ...       ...     ...  ...             ...          ...       ...\n217      2       NaN       5  ...            None          5.0 -3.622288\n218      2       NaN       6  ...            None          6.0 -5.042901\n219      2       NaN       7  ...            None          7.0  1.626482\n220      2       NaN       8  ...            None          8.0  1.711936\n221      2       NaN       9  ...            None          9.0  2.149850\n\n[222 rows x 8 columns]\n\n\n","type":"TEXT"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":"[{\"raw\":{\"height\":300,\"lastColumns\":[],\"version\":1}}]","hideInIFrame":false,"selectedVisualization":"raw","title":"Build a 2-layer neural network using the original data","message":["%python","","try:","    oml.drop(model = 'NN_DIGITS_MODEL') ","except:","    pass","","settings = {'NNET_HIDDEN_LAYERS':'2',","            'NNET_NODES_PER_LAYER':'8,4',","            'NNET_ACTIVATIONS':\"'NNET_ACTIVATIONS_BIPOLAR_SIG','NNET_ACTIVATIONS_TANH'\"}","","nn_mod_digits = oml.nn('classification', **settings)","nn_mod_digits.fit(DIGITS_TRAIN_X, DIGITS_TRAIN_Y, case_id = 'ID', model_name = 'NN_DIGITS_MODEL')"],"enabled":true,"result":{"startTime":1737152727480,"interpreter":"python.low","endTime":1737152732076,"results":[{"message":"\nModel Name: NN_DIGITS_MODEL\n\nModel Owner: OMLUSER\n\nAlgorithm Name: Neural Network\n\nMining Function: CLASSIFICATION\n\nTarget: Digit\n\nSettings: \n                    setting name                                      setting value\n0                      ALGO_NAME                                ALGO_NEURAL_NETWORK\n1          CLAS_WEIGHTS_BALANCED                                                OFF\n2       LBFGS_GRADIENT_TOLERANCE                                         .000000001\n3            LBFGS_HISTORY_DEPTH                                                 20\n4            LBFGS_SCALE_HESSIAN                         LBFGS_SCALE_HESSIAN_ENABLE\n5               NNET_ACTIVATIONS  'NNET_ACTIVATIONS_BIPOLAR_SIG','NNET_ACTIVATIO...\n6             NNET_HIDDEN_LAYERS                                                  2\n7                NNET_ITERATIONS                                                200\n8           NNET_NODES_PER_LAYER                                                8,4\n9                 NNET_TOLERANCE                                            .000001\n10                  ODMS_DETAILS                                        ODMS_ENABLE\n11  ODMS_MISSING_VALUE_TREATMENT                            ODMS_MISSING_VALUE_AUTO\n12              ODMS_RANDOM_SEED                                                  0\n13                 ODMS_SAMPLING                              ODMS_SAMPLING_DISABLE\n14                     PREP_AUTO                                                 ON\n\nComputed Settings: \n       setting name          setting value\n0  NNET_REGULARIZER  NNET_REGULARIZER_NONE\n\nGlobal Statistics: \n  attribute name attribute value\n0      CONVERGED              NO\n1     ITERATIONS             200\n2     LOSS_VALUE         0.11884\n3       NUM_ROWS            1055\n\nAttributes: \nA0\nA1\nA10\nA11\nA12\nA13\nA14\nA15\nA16\nA17\nA18\nA19\nA2\nA20\nA21\nA22\nA23\nA24\nA25\nA26\nA27\nA28\nA29\nA3\nA30\nA31\nA32\nA33\nA34\nA35\nA36\nA37\nA38\nA39\nA4\nA40\nA41\nA42\nA43\nA44\nA45\nA46\nA47\nA48\nA49\nA5\nA50\nA51\nA52\nA53\nA54\nA55\nA56\nA57\nA58\nA59\nA6\nA60\nA61\nA62\nA63\nA7\nA8\nA9\n\nPartition: NO\n\nTopology: \n\n   HIDDEN_LAYER_ID  NUM_NODE           ACTIVATION_FUNCTION\n0                0         8  NNET_ACTIVATIONS_BIPOLAR_SIG\n1                1         4         NNET_ACTIVATIONS_TANH\n\nWeights: \n\n     LAYER  IDX_FROM  IDX_TO  ... ATTRIBUTE_VALUE TARGET_VALUE    WEIGHT\n0        0       0.0       0  ...            None          NaN -0.179083\n1        0       0.0       1  ...            None          NaN  0.036176\n2        0       0.0       2  ...            None          NaN  0.081140\n3        0       0.0       3  ...            None          NaN  0.073680\n4        0       0.0       4  ...            None          NaN -0.283180\n..     ...       ...     ...  ...             ...          ...       ...\n601      2       NaN       5  ...            None          5.0 -1.210018\n602      2       NaN       6  ...            None          6.0 -2.678064\n603      2       NaN       7  ...            None          7.0 -4.357364\n604      2       NaN       8  ...            None          8.0  9.371522\n605      2       NaN       9  ...            None          9.0  3.394959\n\n[606 rows x 8 columns]\n\n\n","type":"TEXT"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":"Score the corresponding test data using each of these models","message":["%python","","PRED_DIGITS_TRANSFORMED = nn_mod_digits_transformed.predict(DIGITS_TRANSFORMED_TEST, supplemental_cols = DIGITS_TRANSFORMED_TEST[['ID','Digit']])","","PRED_DIGITS = nn_mod_digits.predict(DIGITS_TEST, supplemental_cols = DIGITS_TEST[['ID','Digit']])"],"enabled":true,"result":{"startTime":1737152732591,"interpreter":"python.low","endTime":1737152734182,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":null,"message":["%md","","### Review each dataset"],"enabled":true,"result":{"startTime":1737152734646,"interpreter":"md.low","endTime":1737152735095,"results":[{"message":"<h3 id=\"review-each-dataset\">Review each dataset<\/h3>\n","type":"HTML"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":true,"width":0,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":false,"hideVizConfig":true,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"table","title":null,"message":["%python","","z.show(PRED_DIGITS_TRANSFORMED.head())"],"enabled":true,"result":{"startTime":1737152735566,"interpreter":"python.low","endTime":1737152736657,"results":[{"message":"ID\tDigit\tPREDICTION\n149\t8\t3\n473\t7\t1\n559\t1\t1\n700\t8\t3\n760\t2\t3\n","type":"TABLE"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":6,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":false,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"table","title":null,"message":["%python","","z.show(PRED_DIGITS.head())"],"enabled":true,"result":{"startTime":1737152737139,"interpreter":"python.low","endTime":1737152737701,"results":[{"message":"ID\tDigit\tPREDICTION\n1\t0\t0\n5\t4\t4\n8\t7\t7\n9\t8\t8\n13\t2\t1\n","type":"TABLE"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":6,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":false,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"raw","title":"Compute the mean accuracy of the model using transformed data","message":["%python","","nn_mod_digits_transformed.score(DIGITS_TRANSFORMED_TEST.drop('Digit'), DIGITS_TRANSFORMED_TEST['Digit'])"],"enabled":true,"result":{"startTime":1737152738187,"interpreter":"python.low","endTime":1737152739797,"results":[{"message":"0.429363\n","type":"TEXT"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":6,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"raw","title":"Compute the mean accuracy of the model using original data","message":["%python","","nn_mod_digits.score(DIGITS_TEST.drop('Digit'), DIGITS_TEST['Digit'])"],"enabled":true,"result":{"startTime":1737152740296,"interpreter":"python.low","endTime":1737152740934,"results":[{"message":"0.842318\n","type":"TEXT"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":6,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"raw","title":"Display the confusion matrix for the scored transformed data","message":["%python","","cm_data_transformed = PRED_DIGITS_TRANSFORMED.crosstab('Digit','PREDICTION').pull()","","df = pd.DataFrame(cm_data_transformed)","","cm = np.zeros((10, 10), dtype=int)","for _, row in df.iterrows():","    digit = row['Digit']","    prediction = row['PREDICTION']","    count = row['count']","    cm[digit][prediction] = count","","labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']","","print('Confusion Matrix:')","print('\\t' + '\\t'.join(labels))","","for i, row in enumerate(cm):","    print(labels[i], end='\\t')","    for count in row:","        print(count, end='\\t')","    print()"],"enabled":true,"result":{"startTime":1737152741406,"interpreter":"python.low","endTime":1737152743498,"results":[{"message":"Confusion Matrix:\n\t0\t1\t2\t3\t4\t5\t6\t7\t8\t9\n0\t52\t4\t0\t6\t0\t0\t0\t8\t1\t7\t\n1\t6\t47\t0\t12\t2\t0\t0\t5\t1\t2\t\n2\t0\t8\t23\t25\t0\t0\t0\t4\t3\t0\t\n3\t1\t18\t5\t35\t1\t1\t0\t8\t1\t5\t\n4\t0\t31\t0\t3\t28\t0\t0\t10\t3\t0\t\n5\t1\t5\t2\t28\t1\t9\t0\t19\t12\t4\t\n6\t0\t14\t0\t6\t12\t2\t30\t0\t3\t14\t\n7\t0\t7\t1\t12\t1\t0\t0\t40\t1\t0\t\n8\t0\t10\t0\t17\t2\t2\t0\t7\t22\t6\t\n9\t2\t13\t0\t17\t0\t0\t0\t1\t9\t24\t\n","type":"TEXT"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"raw","title":"Display the confusion matrix for the scored original data","message":["%python","","cm_data = PRED_DIGITS.crosstab('Digit','PREDICTION').pull()","","df = pd.DataFrame(cm_data)","","cm = np.zeros((10, 10), dtype=int)","for _, row in df.iterrows():","    digit = row['Digit']","    prediction = row['PREDICTION']","    count = row['count']","    cm[digit][prediction] = count","","labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']","","print('Confusion Matrix:')","print('\\t' + '\\t'.join(labels))","","for i, row in enumerate(cm):","    print(labels[i], end='\\t')","    for count in row:","        print(count, end='\\t')","    print()"],"enabled":true,"result":{"startTime":1737152743976,"interpreter":"python.low","endTime":1737152744648,"results":[{"message":"Confusion Matrix:\n\t0\t1\t2\t3\t4\t5\t6\t7\t8\t9\n0\t65\t0\t0\t0\t1\t0\t0\t0\t6\t4\t\n1\t0\t64\t0\t0\t0\t0\t0\t0\t4\t1\t\n2\t0\t6\t64\t1\t0\t1\t0\t0\t5\t0\t\n3\t0\t0\t3\t68\t0\t4\t0\t2\t2\t1\t\n4\t0\t1\t0\t0\t64\t0\t0\t0\t0\t0\t\n5\t0\t0\t0\t1\t1\t69\t0\t0\t2\t9\t\n6\t0\t3\t0\t0\t0\t0\t66\t0\t7\t0\t\n7\t1\t0\t0\t1\t1\t0\t0\t60\t3\t3\t\n8\t2\t7\t1\t0\t2\t4\t4\t2\t47\t3\t\n9\t1\t0\t0\t2\t0\t9\t0\t5\t1\t58\t\n","type":"TEXT"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":null,"title":"Clean up","message":["%python","","oml.drop('IRIS_NMF')","oml.drop('DIGITS_NMF')"],"enabled":false,"result":null,"sizeX":0,"hideCode":false,"width":12,"hideResult":false,"dynamicFormParams":null,"row":0,"hasTitle":true,"hideVizConfig":false,"hideGutter":true,"relations":[],"forms":null},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":"html","title":null,"message":["%md","# End of Script"],"enabled":true,"result":{"startTime":1737152745137,"interpreter":"md.low","endTime":1737152745592,"results":[{"message":"<h1 id=\"end-of-script\">End of Script<\/h1>\n","type":"HTML"}],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":true,"width":12,"hideResult":false,"dynamicFormParams":"{}","row":0,"hasTitle":false,"hideVizConfig":true,"hideGutter":true,"relations":[],"forms":"[]"},{"col":0,"visualizationConfig":null,"hideInIFrame":false,"selectedVisualization":null,"title":null,"message":[],"enabled":true,"result":{"startTime":1737152746087,"interpreter":"md.low","endTime":1737152746564,"results":[],"taskStatus":"SUCCESS","forms":"[]","status":"SUCCESS"},"sizeX":0,"hideCode":true,"width":0,"hideResult":true,"dynamicFormParams":"{}","row":0,"hasTitle":false,"hideVizConfig":true,"hideGutter":true,"relations":[],"forms":"[]"}],"version":"6","snapshot":false,"tags":null}]